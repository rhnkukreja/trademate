This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.gitignore
DockerFile
live_core_functions/build_monitor_list.py
live_core_functions/live_paper_trader.py
live_core_functions/minute_monitor_stocks.py
live_core_functions/monitor_breakouts.py
live_core_functions/real_money_trading.py
llm functions/decisons_by_ai.py
main.py
past_data_fetch/kite_algo_breakouts.py
past_data_fetch/paper_trading.py
requirements.txt
utils/common.py
utils/news_agent.py
utils/refresh_token.py
utils/screenshot_scrapper.py
utils/slack_bot.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="live_core_functions/real_money_trading.py">
import datetime
import time
import threading
import pandas as pd
import gspread
import os
import json
from dotenv import load_dotenv
from utils.common import kite, logger, next_price_above, load_token_map, token_map
from live_core_functions.build_monitor_list import main as run_monitor_builder
from live_core_functions.minute_monitor_stocks import get_stocks_by_tier, run_breakout_check, ARMED_SYMBOLS
from live_core_functions.monitor_breakouts import process_breakout, get_monitor_list
from live_core_functions.live_paper_trader import ACTIVE_EXIT_MONITORS, finalize_trade

load_dotenv()
# --- Google Sheets Setup ---
SHEET_LOCK = threading.Lock()
gc = None 
sheet = None
creds_json = os.getenv("GOOGLE_SHEET_CREDS")

if creds_json:
    try:
        info = json.loads(creds_json)
        gc = gspread.service_account_info(info)
        sh = gc.open("Live_Paper_Trading")
        sheet = sh.sheet1
        logger.info("‚úÖ Successfully connected to Google Sheets")
    except Exception as e:
        logger.error(f"‚ùå Failed to load Google Sheet: {e}")
    
    # Get Sheet8 directly by name
    sheet = sh.worksheet("Sheet8")
    
    logger.info(f"‚úÖ Connected to Google Sheet: {sheet.title}")

# Track armed times for real money trading
ARMED_TIMES_REAL = {}

def update_sheet_on_breakout(symbol, breakout_time, armed_time, buy_price, breakout_price, quantity):
    """
    Updates Google Sheet when a breakout buy order executes.
    Fills columns A through I.
    """
    if sheet is None:
        logger.error(f"‚ùå Sheet not connected. Cannot log {symbol}")
        return False
    
    money_traded = round(quantity * buy_price, 2)
    
    row = [
        breakout_time,      # A: Breakout Time
        symbol,             # B: Symbol
        armed_time,         # C: Armed Time
        breakout_time,      # D: Buy Time (same as breakout for SL-L orders)
        buy_price,          # E: Buy Price
        breakout_price,     # F: Breakout Price
        quantity,           # G: Quantity
        money_traded,       # H: Money Traded
        "OPEN",             # I: Status
        "",                 # J: Selling Price (empty, filled on exit)
        "",                 # K: Exit Reason (empty, filled on exit)
        ""                  # L: P&L % (empty, filled on exit)
    ]
    
    try:
        with SHEET_LOCK:
            sheet.append_row(row)
        logger.info(f"‚úÖ Sheet updated for {symbol} | Buy={buy_price} | Qty={quantity} | Money=‚Çπ{money_traded}")
        return True
    except Exception as e:
        logger.error(f"‚ùå Sheet update failed for {symbol}: {e}")
        return False


def update_sheet_on_exit(symbol, selling_price, exit_reason):
    """
    Finds the OPEN trade row for this symbol and fills exit details.
    Updates columns I, J, K, L.
    """
    if sheet is None:
        logger.error(f"‚ùå Sheet not connected. Cannot update exit for {symbol}")
        return False
    
    try:
        all_rows = sheet.get_all_values()
        
        for i, row in enumerate(all_rows[1:], start=2):  # Skip header, gspread is 1-indexed
            # Find row where Symbol matches AND Status is OPEN
            if len(row) >= 9 and row[1] == symbol and row[8].strip() == "OPEN":
                buy_price = float(row[4])  # Column E
                pnl_pct = round(((selling_price - buy_price) / buy_price) * 100, 2)
                
                with SHEET_LOCK:
                    sheet.update_cell(i, 9, "CLOSED")                # I: Status
                    sheet.update_cell(i, 10, selling_price)          # J: Selling Price
                    sheet.update_cell(i, 11, exit_reason)            # K: Exit Reason
                    sheet.update_cell(i, 12, pnl_pct / 100)          # L: P&L %
                
                logger.info(f"‚úÖ Sheet exit updated for {symbol} | Sold={selling_price} | Reason={exit_reason} | P&L={pnl_pct}%")
                return True
        
        logger.warning(f"‚ö†Ô∏è No OPEN trade found for {symbol} in sheet")
        return False
        
    except Exception as e:
        logger.error(f"‚ùå Sheet exit update failed for {symbol}: {e}")
        return False

ARMED_SYMBOLS_REAL = set()
LOGGED_SKIPS = set()

# --- CONFIGURATION ---
LIVE_TRADING = True
TARGET_PCT = 1.03
EXIT_TIME = datetime.time(15, 15)
DAILY_BUDGET = 2400            # Total budget for the day
daily_spent = 0                # Tracks how much has been spent

# Time-based expected breakout slots
# Format: (start_time, end_time, expected_breakouts_remaining)
BREAKOUT_SLOTS = [
    (datetime.time(9, 15),  datetime.time(11, 0),  3),   # Morning: assume 3 more
    (datetime.time(11, 0),  datetime.time(13, 0),  2),   # Midday: assume 2 more
    (datetime.time(13, 0),  datetime.time(15, 30), 1),   # Afternoon: assume 1 more
]

def get_investment_for_current_trade():
    """
    Dynamically calculates how much to invest in the current trade
    based on remaining budget and time of day.
    
    Logic:
    - Divides remaining budget by expected remaining breakouts
    - Expected breakouts decreases as the day progresses
    - If budget already spent, returns 0
    """
    remaining_budget = DAILY_BUDGET - daily_spent
    
    # No budget left
    if remaining_budget <= 0:
        logger.warning(f"‚ùå Budget exhausted: ‚Çπ{daily_spent}/‚Çπ{DAILY_BUDGET}")
        return 0
    
    # Find current time slot
    now = datetime.datetime.now().time()
    expected_remaining = 1  # Default: assume at least 1 breakout
    
    for start, end, slots in BREAKOUT_SLOTS:
        if start <= now < end:
            expected_remaining = slots
            break
    
    # Calculate investment for this trade
    investment = remaining_budget / expected_remaining
    
    logger.info(
        f"üí∞ Budget Calc: "
        f"Remaining=‚Çπ{remaining_budget:.2f} | "
        f"Expected Slots={expected_remaining} | "
        f"This Trade=‚Çπ{investment:.2f}"
    )
    
    return investment

def get_live_allocation_qty(price):
    """
    Calculates quantity based on dynamic investment amount.
    Also verifies actual account balance before placing order.
    """
    
    try:
        # 1. Get dynamic investment amount for this trade
        investment = get_investment_for_current_trade()
        if investment <= 0:
            return 0
        
        # 2. Verify actual account balance
        margins = kite.margins()
        available_cash = margins["equity"]["available"]["live_balance"]
        
        # Use the LOWER of: dynamic investment OR available cash
        actual_investment = min(investment, available_cash)
        
        if actual_investment <= 0:
            logger.warning(f"‚ùå No funds available: Balance=‚Çπ{available_cash:.2f}")
            return 0
        
        # 3. Calculate quantity
        qty = int(actual_investment / price)

        # --- REFINED LOGIC TO IGNORE EXPENSIVE STOCKS ---
        if actual_investment < price:
            logger.info(f"‚è≠Ô∏è Ignoring: Stock price (‚Çπ{price}) exceeds allocated investment (‚Çπ{actual_investment:.2f})")
            return 0
        
        if qty <= 0:
            logger.warning(f"‚ùå Qty is 0: Price ‚Çπ{price} too high for ‚Çπ{actual_investment:.2f}")
            return 0
        
        # 4. Calculate real investment (qty * price)
        real_investment = qty * price
        
        # 5. Update daily spent
        
        logger.info(
            f"üí∞ Order Details: "
            f"Qty={qty} | "
            f"Price=‚Çπ{price} | "
            f"Investment=‚Çπ{real_investment:.2f} | "
            f"Daily: ‚Çπ{daily_spent:.2f}/‚Çπ{DAILY_BUDGET}"
        )
        
        return qty
        
    except Exception as e:
        logger.error(f"Error in get_live_allocation_qty: {e}")
        return 0

def place_real_order(symbol, trigger_price, tick_size):
    """
    Places SL-L order for real money trading.
    This is an ADVANCE order that executes when price hits trigger_price.
    """
    if not LIVE_TRADING:
        logger.info(f"[SIMULATION] ARM SL-L ORDER: {symbol} | Trigger={trigger_price} Limit={round(trigger_price + tick_size, 2)}")
        return "SIM_ID"

    limit_price = round(trigger_price + tick_size, 2)
    qty = get_live_allocation_qty(trigger_price)
    
    if qty <= 0:
        logger.error(f"‚ùå Cannot arm {symbol}: Insufficient funds for qty")
        return None

    try:
        # Check current LTP to ensure we're not already past trigger
        quote = kite.quote(f"NSE:{symbol}")
        ltp = quote[f"NSE:{symbol}"]["last_price"]
        
        if ltp >= trigger_price:
            logger.warning(f"‚ö†Ô∏è Skip arming {symbol}: LTP {ltp} already >= trigger {trigger_price}")
            return None
        
        # Place SL-L order
        order_id = kite.place_order(
            variety=kite.VARIETY_REGULAR,
            exchange=kite.EXCHANGE_NSE,
            tradingsymbol=symbol,
            transaction_type=kite.TRANSACTION_TYPE_BUY,
            quantity=qty,
            order_type=kite.ORDER_TYPE_SL,
            product=kite.PRODUCT_MIS,
            price=limit_price,
            trigger_price=trigger_price,
            validity=kite.VALIDITY_DAY
        )

        # ONLY update budget IF order_id is returned successfully
        if order_id:
            global daily_spent
            daily_spent += (qty * trigger_price)
            logger.info(f"‚úÖ ARMED SL-L ORDER: {symbol} | OrderID={order_id} | Spent: ‚Çπ{daily_spent}")
            return order_id
    except Exception as e:
        logger.error(f"‚ùå Order Failed for {symbol}: {e}")
        return None
    
def monitor_order_status(symbol, order_id, breakout_price):
    """
    Monitors an armed SL-L order and starts exit monitoring when it executes.
    Runs in background thread.
    """
    logger.info(f"üì° Monitoring order status for {symbol} (OrderID: {order_id})")
    
    max_wait_time = 3600  # 1 hour max wait
    start_time = time.time()
    check_interval = 5  # Check every 5 seconds
    
    while True:
        try:
            # Check if we've waited too long
            if time.time() - start_time > max_wait_time:
                logger.warning(f"‚è∞ Order monitoring timeout for {symbol} after 1 hour")
                # Cancel the order
                try:
                    kite.cancel_order(variety=kite.VARIETY_REGULAR, order_id=order_id)
                    logger.info(f"üóëÔ∏è Cancelled stale order for {symbol}")
                except:
                    pass
                ARMED_SYMBOLS_REAL.discard(symbol)
                break
            
            # Check if market is closed
            now = datetime.datetime.now()
            if now.time() >= datetime.time(15, 30):
                logger.info(f"üîî Market closed, cancelling order for {symbol}")
                try:
                    kite.cancel_order(variety=kite.VARIETY_REGULAR, order_id=order_id)
                except:
                    pass
                ARMED_SYMBOLS_REAL.discard(symbol)
                break
            
            # Get order status
            order_info = kite.order_history(order_id)
            if not order_info:
                time.sleep(check_interval)
                continue
            
            latest_status = order_info[-1]['status']
            
            if latest_status == 'COMPLETE':
                avg_price = order_info[-1]['average_price']
                filled_qty = order_info[-1]['filled_quantity']
                logger.info(f"‚úÖ ORDER EXECUTED: {symbol} at ‚Çπ{avg_price} | Qty={filled_qty}")
                
                # ============================================
                # UPDATE GOOGLE SHEET
                # ============================================
                breakout_time_str = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                armed_time_str = ARMED_TIMES_REAL.get(symbol, "")
                
                update_sheet_on_breakout(
                    symbol=symbol,
                    breakout_time=breakout_time_str,
                    armed_time=armed_time_str,
                    buy_price=avg_price,
                    breakout_price=breakout_price,
                    quantity=filled_qty
                )
                
                # Start exit monitoring
                ACTIVE_EXIT_MONITORS.add(symbol)
                threading.Thread(
                    target=monitor_live_exit,
                    args=(symbol, avg_price, filled_qty),
                    daemon=True
                ).start()
                
                ARMED_SYMBOLS_REAL.discard(symbol)
                break
                
            elif latest_status in ['CANCELLED', 'REJECTED']:
                logger.warning(f"‚ùå Order {latest_status} for {symbol}")
                ARMED_SYMBOLS_REAL.discard(symbol)
                break
            
            # Order still pending, keep waiting
            time.sleep(check_interval)
            
        except Exception as e:
            logger.error(f"Error monitoring order for {symbol}: {e}")
            time.sleep(check_interval)

def monitor_live_exit(symbol, buy_price, quantity, target_order_id=None):
    """
    Monitors live exit conditions for real money trades.
    
    Args:
        symbol: Stock symbol
        buy_price: Price at which we bought
        quantity: Number of shares we bought
        target_order_id: Not used currently, kept for future use
    """
    token = token_map.get(symbol)
    if not token:
        logger.error(f"‚ùå Token not found for {symbol}, cannot monitor exit")
        return
    
    sl = buy_price                      # Stop Loss = Buy Price (cost)
    target = round(buy_price * TARGET_PCT, 2)  # Target = Buy Price + 3%
    
    logger.info(f"üì° Exit monitor started: {symbol} | Buy={buy_price} | SL={sl} | Target={target} | Qty={quantity}")
    
    # --- MONITORING LOOP ---
    while symbol in ACTIVE_EXIT_MONITORS:
        try:
            now = datetime.datetime.now()
            
            # ============================================
            # EXIT 1: EOD EXIT at 15:15
            # ============================================
            if now.time() >= EXIT_TIME:
                logger.info(f"üîî EOD time reached for {symbol}, placing market sell...")
                
                try:
                    kite.place_order(
                        variety=kite.VARIETY_REGULAR,
                        exchange=kite.EXCHANGE_NSE,
                        tradingsymbol=symbol,
                        transaction_type=kite.TRANSACTION_TYPE_SELL,
                        quantity=quantity,                          # ‚úÖ CORRECT quantity
                        order_type=kite.ORDER_TYPE_MARKET,
                        product=kite.PRODUCT_MIS
                    )
                    
                    # Get actual exit price from latest candle
                    candles = kite.historical_data(token, now.date(), now, "minute")
                    exit_price = round(candles[-1]['close'], 2) if candles else buy_price
                    
                    # ‚úÖ Update real money sheet
                    update_sheet_on_exit(symbol, exit_price, "EOD Exit @15:15")
                    logger.info(f"üîî EOD EXIT: {symbol} | Sold at ‚Çπ{exit_price}")
                    
                except Exception as e:
                    logger.error(f"‚ùå EOD sell order failed for {symbol}: {e}")
                    # Still update sheet with buy_price as fallback
                    update_sheet_on_exit(symbol, buy_price, "EOD Exit (order failed)")
                
                break
            
            # ============================================
            # FETCH CURRENT PRICE (Using Quotes))
            # ============================================
            try:
                quote = kite.quote(f"NSE:{symbol}")
                curr_price = quote[f"NSE:{symbol}"]["last_price"]
            except Exception as e:
                logger.error(f"Quote fetch failed for {symbol}: {e}")
                time.sleep(5)
                continue
            
            # ============================================
            # EXIT 2: STOP LOSS HIT (Price <= Buy Price)
            # ============================================
            if curr_price <= sl:
                logger.info(f"‚ùå SL triggered for {symbol} | Current={curr_price} | SL={sl}")
                
                try:
                    kite.place_order(
                        variety=kite.VARIETY_REGULAR,
                        exchange=kite.EXCHANGE_NSE,
                        tradingsymbol=symbol,
                        transaction_type=kite.TRANSACTION_TYPE_SELL,
                        quantity=quantity,                          # ‚úÖ CORRECT quantity
                        order_type=kite.ORDER_TYPE_MARKET,
                        product=kite.PRODUCT_MIS
                    )
                    
                    # Get actual exit price
                    candles = kite.historical_data(token, now.date(), now, "minute")
                    exit_price = round(candles[-1]['close'], 2) if candles else sl
                    
                    # ‚úÖ Update real money sheet
                    update_sheet_on_exit(symbol, exit_price, "SL Hit Breakout Price")
                    logger.info(f"‚ùå SL EXIT: {symbol} | Sold at ‚Çπ{exit_price}")
                    
                except Exception as e:
                    logger.error(f"‚ùå SL sell order failed for {symbol}: {e}")
                    update_sheet_on_exit(symbol, sl, "SL Hit (order failed)")
                
                break
            
            # ============================================
            # EXIT 3: TARGET HIT (Price >= Buy Price + 3%)
            # ============================================
            elif curr_price >= target:
                logger.info(f"üéØ Target triggered for {symbol} | Current={curr_price} | Target={target}")
                
                try:
                    kite.place_order(
                        variety=kite.VARIETY_REGULAR,
                        exchange=kite.EXCHANGE_NSE,
                        tradingsymbol=symbol,
                        transaction_type=kite.TRANSACTION_TYPE_SELL,
                        quantity=quantity,                          # ‚úÖ CORRECT quantity
                        order_type=kite.ORDER_TYPE_MARKET,
                        product=kite.PRODUCT_MIS
                    )
                    
                    # Get actual exit price
                    candles = kite.historical_data(token, now.date(), now, "minute")
                    exit_price = round(candles[-1]['close'], 2) if candles else target
                    
                    # ‚úÖ Update real money sheet
                    update_sheet_on_exit(symbol, exit_price, "Target Hit 3%")
                    logger.info(f"üéØ TARGET EXIT: {symbol} | Sold at ‚Çπ{exit_price}")
                    
                except Exception as e:
                    logger.error(f"‚ùå Target sell order failed for {symbol}: {e}")
                    update_sheet_on_exit(symbol, target, "Target Hit (order failed)")
                
                break
            
            time.sleep(5)
            
        except Exception as e:
            logger.error(f"Error monitoring exit for {symbol}: {e}")
            time.sleep(5)

    # --- RE-ENTRY CLEANUP ---
    ACTIVE_EXIT_MONITORS.discard(symbol)
    if symbol in ARMED_SYMBOLS_REAL:
        ARMED_SYMBOLS_REAL.discard(symbol)
    # Clean up armed time tracking
    ARMED_TIMES_REAL.pop(symbol, None)
    logger.info(f"‚ôªÔ∏è Re-entry enabled for {symbol}")

def process_stocks(symbols, today, tier):
    """
    Processes a list of stocks for potential arming.
    Handles both SLOW and FAST tier stocks.
    
    Args:
        symbols: List of stock symbols to check
        today: Current date
        tier: "slow" or "fast" (for logging purposes)
    """
    
    # ============================================
    # OPTIMIZATION: Fetch ALL stock data in ONE call
    # ============================================
    all_stock_data = get_monitor_list(today, symbols=symbols)
    
    # Create a dictionary for fast lookup
    stock_data_map = {stock["symbol"]: stock for stock in all_stock_data}
    
    logger.info(f"üìä {tier.upper()} tier: Fetched {len(stock_data_map)} stocks from monitor list")
    
    # ============================================
    # Fetch ALL quotes in batches of 50 (Kite limit)
    # ============================================
    all_quotes = {}
    for i in range(0, len(symbols), 50):
        batch = symbols[i:i+50]
        try:
            batch_keys = [f"NSE:{s}" for s in batch]
            quotes = kite.quote(batch_keys)
            all_quotes.update(quotes)
        except Exception as e:
            logger.error(f"Quote batch failed: {e}")
    
    # ============================================
    # Now process each symbol with pre-fetched data
    # ============================================
    for symbol in symbols:
        if datetime.datetime.now().time() >= datetime.time(15, 10):
            logger.info("üïí Past 15:10 - Stopping new entry checks for today.")
            break # Use break instead of continue to stop the whole loop faster

        # Skip if already in active trade
        if symbol in ACTIVE_EXIT_MONITORS: 
            continue
        
        # Skip if already armed
        if symbol in ARMED_SYMBOLS_REAL:
            continue
        
        # Get stock data from map (no database call)
        stock_data = stock_data_map.get(symbol)
        if not stock_data:
            continue

        # Get quote from batch fetch (no API call)
        quote = all_quotes.get(f"NSE:{symbol}")
        if not quote:
            continue
        
        current_price = quote["last_price"]

        mas = []
        for p in [20, 50, 100, 200]:
            for t in ['daily', 'hourly']:
                val = stock_data.get(f"ma{p}_{t}")
                if val is not None:
                    mas.append(val)
        
        if len(mas) != 8:
            continue
        
        mas.sort()

        # ============================================
        # TIER UPGRADE: SLOW ‚Üí FAST
        # ============================================
        if tier == "slow":
            sixth_ma = mas[-3]  # 6th MA (3rd highest)
        
            # If price crosses 6th MA, upgrade to FAST
            if current_price >= sixth_ma:
                try:
                    from utils.common import supabase
                    supabase.table("monitor_list").update(
                        {"monitoring_tier": "fast"}
                    ).eq("symbol", symbol).eq("date", today.strftime("%Y-%m-%d")).execute()
                    
                    logger.info(f"‚ö° {symbol} upgraded to FAST | LTP {current_price} >= 6th MA {sixth_ma}")
                except Exception as e:
                    logger.error(f"Failed to upgrade {symbol}: {e}")
                
                # Don't arm yet - wait for next FAST tier check
                continue

        # ============================================
        # TIER DOWNGRADE: FAST ‚Üí SLOW
        # ============================================
        if tier == "fast":
            # Identify the 5th MA (out of 8). If price falls below it, the stock is 'cooling off'.
            fifth_ma = mas[-4] if len(mas) >= 4 else mas[0]
            
            if current_price <= fifth_ma:
                try:
                    from utils.common import supabase
                    # Update the database so the script checks this stock less frequently
                    supabase.table("monitor_list").update(
                        {"monitoring_tier": "slow"}
                    ).eq("symbol", symbol).eq("date", today.strftime("%Y-%m-%d")).execute()
                    
                    logger.info(f"üê¢ {symbol} downgraded to SLOW | LTP {current_price} <= 5th MA {fifth_ma}")
                except Exception as e:
                    logger.error(f"Failed to downgrade {symbol}: {e}")

        # ============================================
        # ARMING LOGIC: Only for FAST tier OR if already at midpoint
        # ============================================
        
        max_ma = mas[-1]      # 8th MA (highest)
        seventh_ma = mas[-2]  # 7th MA (2nd highest)
        
        tick_size = stock_data.get("tick_size", 0.05)
        
        # Calculate midpoint and arm threshold
        midpoint = (seventh_ma + max_ma) / 2
        arm_threshold = midpoint - tick_size
        
        # Check if price reached arming point
        if current_price >= arm_threshold:
            breakout_price = next_price_above(max_ma, tick_size)

            if (breakout_price - current_price) < (tick_size * 2):
                logger.warning(f"‚è© {symbol} too close to breakout ({current_price} vs {breakout_price}). Skipping advance arming.")
                LOGGED_SKIPS.add(symbol)
                continue
            
            # Place advance SL-L order
            order_id = place_real_order(symbol, breakout_price, tick_size)
            
            if order_id:
                ARMED_SYMBOLS_REAL.add(symbol)
                ARMED_TIMES_REAL[symbol] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                
                # Start monitoring order status in background
                threading.Thread(
                    target=monitor_order_status,
                    args=(symbol, order_id, breakout_price),
                    daemon=True
                ).start()
                
                logger.info(f"üü¶ ARMED: {symbol} ({tier.upper()}) | Current={current_price:.2f} | Midpoint={midpoint:.2f} | Breakout={breakout_price:.2f}")

def live_engine():
    global daily_spent
    daily_spent = 0
    
    today = datetime.date.today()
    load_token_map()
    last_fast_check = None
    last_slow_check = None
    
    logger.info(f"üöÄ Starting live engine | Daily Budget: ‚Çπ{DAILY_BUDGET}")
    
    # Flag to prevent overlapping slow checks
    slow_check_running = False
    
    def run_slow_tier_background():
        """Runs SLOW tier checks in background thread"""
        nonlocal slow_check_running, last_slow_check
        try:
            slow_check_running = True
            slow_stocks = get_stocks_by_tier("slow", today)
            logger.info(f"üü¢ SLOW tier: Checking {len(slow_stocks)} stocks")
            
            if slow_stocks:
                process_stocks(slow_stocks, today, "slow")
            
            last_slow_check = datetime.datetime.now()
        except Exception as e:
            logger.error(f"Error in SLOW tier thread: {e}")
        finally:
            slow_check_running = False
    
    while True:
        now = datetime.datetime.now()
        
        # Only run during market hours
        if now.time() < datetime.time(9, 15) or now.time() > datetime.time(15, 30):
            logger.info("Market closed. Sleeping...")
            time.sleep(60)
            continue
        
        # ============================================
        # 1. FAST TIER: Check every 60 seconds
        # ============================================
        if last_fast_check is None or (now - last_fast_check).seconds >= 60:
            fast_stocks = get_stocks_by_tier("fast", today)
            
            if fast_stocks:
                logger.debug(f"‚ö° FAST tier: Checking {len(fast_stocks)} stocks")
                process_stocks(fast_stocks, today, "fast")
            
            last_fast_check = now
        
        # ============================================
        # 2. SLOW TIER: Check every 15 minutes (900 seconds)
        # ============================================
        if (last_slow_check is None or (now - last_slow_check).total_seconds() >= 900) and not slow_check_running:
            # Update timestamp BEFORE starting to prevent race conditions
            last_slow_check = now
            t = threading.Thread(target=run_slow_tier_background)
            t.daemon = True
            t.start()
        
        time.sleep(5)

if __name__ == "__main__":
    live_engine()
</file>

<file path="llm functions/decisons_by_ai.py">
import numpy as np
import datetime
import time
import pandas as pd
import subprocess
import base64
import google.generativeai as genai
import os
import requests
import joblib
from sklearn.linear_model import LinearRegression
from utils.common import load_token_map, token_map
from utils.common import kite, supabase, logger

load_token_map()
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
NIFTY_TOKEN = 256265

ML_BUNDLE = joblib.load("logistic_breakout_model_2.pkl")
ML_MODEL = ML_BUNDLE["model"]
ML_SCALER = ML_BUNDLE["scaler"]
ML_IMPUTER = ML_BUNDLE["imputer"]
ML_FEATURES = ML_BUNDLE["features"]
    
def run_ml_prediction(feature_dict):
    X = pd.DataFrame([feature_dict])[ML_FEATURES]
    X = ML_IMPUTER.transform(X)
    X = ML_SCALER.transform(X)
    prob = ML_MODEL.predict_proba(X)[0][1]
    return prob

def evaluate_breakout_with_ai(symbol, instrument_token, breakout_time, breakout_data):

    stock_df = fetch_30d_daily_ohlcv(
        symbol,
        instrument_token,
        breakout_time.date()
    )

    if stock_df is None:
        return None, "NO_STOCK_DATA"

    nifty_map = fetch_nifty_daily_map(breakout_time.date())

    rows = []
    for _, r in stock_df.iterrows():
        n = nifty_map.get(r["date"])
        rows.append({
            "open": r["open"],
            "high": r["high"],
            "low": r["low"],
            "close": r["close"],
            "volume": r["volume"],
            "nifty_open": n["open"] if n else 0,
            "nifty_close": n["close"] if n else 0
        })

    ml_df = pd.DataFrame(rows)

    features = get_ml_model_features(
        ml_df,
        ml_df.rename(columns={
            "nifty_open":"nifty_open",
            "nifty_close":"nifty_close"
        })
    )

    try:
        ml_prob = run_ml_prediction(features)
    except Exception as e:
        logger.error(f"ML prediction failed: {e}")
        ml_prob = None


    charts = capture_tv_charts(symbol, breakout_time)
    if not charts:
        return ml_prob, "NO_CHARTS"

    ai_decision, prompt = run_ai_evaluation(breakout_data, charts)

    return ml_prob, ai_decision

def capture_tv_charts(symbol, breakout_time):
    """Calls the screenshot scrapper and returns paths + base64 images."""

    dt_string = breakout_time.strftime("%Y-%m-%dT%H:%M:%S")

    day_num = breakout_time.day
    suffix = "th" if 11 <= day_num <= 13 else {1: "st", 2: "nd", 3: "rd"}.get(day_num % 10, "th")
    date_str = f"{day_num}{suffix}{breakout_time.strftime('%b%Y')}"
    safe_symbol = symbol.replace(":", "_")

    out_dir = f"./chart_screens/{safe_symbol}_{date_str}"
    os.makedirs(out_dir, exist_ok=True)

    # Call your screenshot script
    try:
        result = subprocess.run([
            "python", "screenshot_scrapper.py",
            "--symbol", symbol,
            "--dt", dt_string,
            "--out", out_dir,
            "--headless"
        ], timeout=180, capture_output=True, text=True)
        
        if result.returncode != 0:
            logger.error(f"Screenshot failed for {symbol}: {result.stderr}")
            return None
            
    except subprocess.TimeoutExpired:
        logger.error(f"Screenshot timeout for {symbol}")
        return None
    except Exception as e:
        logger.error(f"Screenshot error for {symbol}: {e}")
        return None

    # Build file paths
    img_15m = f"{out_dir}/{safe_symbol}_{date_str}_15m.png"
    img_1h  = f"{out_dir}/{safe_symbol}_{date_str}_1h.png"
    img_D   = f"{out_dir}/{safe_symbol}_{date_str}_D.png"

    # Check if files exist before encoding
    if not all(os.path.exists(p) for p in [img_15m, img_1h, img_D]):
        logger.error(f"Screenshot files missing for {symbol}")
        return None

    # Convert to base64
    def encode(path):
        with open(path, "rb") as f:
            return base64.b64encode(f.read()).decode()

    return {
        "img_15m": encode(img_15m),
        "img_1h": encode(img_1h),
        "img_D": encode(img_D),
        "paths": [img_15m, img_1h, img_D]
    }
    
def calculate_rsi(series, period=14):
    """Calculate RSI manually using pandas."""
    delta = series.diff()
    gain = delta.where(delta > 0, 0.0)
    loss = -delta.where(delta < 0, 0.0)
    avg_gain = gain.rolling(window=period, min_periods=period).mean()
    avg_loss = loss.rolling(window=period, min_periods=period).mean()
    rs = avg_gain / avg_loss
    rsi = 100 - (100 / (1 + rs))
    return rsi


def get_historical_context(symbol, instrument_token, analysis_date):
    """
    Fetches past 30 trading days of OHLCV for the stock and Nifty.
    Adapted from build_ohlcv_dataset.py logic.
    """
    # 45 days buffer to ensure we get 30 trading days
    start_date = analysis_date - datetime.timedelta(days=45)
    end_date = analysis_date - datetime.timedelta(days=1)
    
    try:
        # 1. Fetch Stock Daily Data
        candles = kite.historical_data(instrument_token, start_date, end_date, "day")
        if not candles:
            return None, None
            
        df = pd.DataFrame(candles).tail(30)
        
        # 2. Fetch Nifty Daily Data
        nifty_token = token_map.get("NIFTY")
        nifty_candles = kite.historical_data(nifty_token, start_date, end_date, "day")
        n_df = pd.DataFrame(nifty_candles)
        
        # Create a mapping for easy Nifty lookup by date
        n_df["date"] = pd.to_datetime(n_df["date"]).dt.date
        nifty_map = n_df.set_index("date")[["open", "close"]].to_dict('index')

        # Format for AI Prompt context
        df["date_str"] = pd.to_datetime(df["date"]).dt.date
        history_rows = []
        for _, r in df.iterrows():
            d = r["date_str"]
            n_row = nifty_map.get(d, {"open": "N/A", "close": "N/A"})
            history_rows.append(
                f"{d}: O:{r['open']} H:{r['high']} L:{r['low']} C:{r['close']} V:{r['volume']} | Nifty_O:{n_row['open']} Nifty_C:{n_row['close']}"
            )
        
        return "\n".join(history_rows), len(df)
    except Exception as e:
        logger.error(f"Error fetching history for {symbol}: {e}")
        return None, 0

def fetch_30d_daily_ohlcv(symbol, instrument_token, breakout_date):
    end_date = breakout_date - datetime.timedelta(days=1)
    start_date = breakout_date - datetime.timedelta(days=45)

    candles = kite.historical_data(
        instrument_token=instrument_token,
        from_date=start_date,
        to_date=end_date,
        interval="day"
    )

    if not candles:
        return None

    df = pd.DataFrame(candles)
    df["date"] = pd.to_datetime(df["date"]).dt.date
    df = df.sort_values("date").tail(30)

    return df if not df.empty else None

def fetch_nifty_daily_map(breakout_date):
    end_date = breakout_date - datetime.timedelta(days=1)
    start_date = breakout_date - datetime.timedelta(days=45)

    candles = kite.historical_data(
        instrument_token=NIFTY_TOKEN,
        from_date=start_date,
        to_date=end_date,
        interval="day"
    )

    df = pd.DataFrame(candles)
    df["date"] = pd.to_datetime(df["date"]).dt.date

    return {
        r["date"]: r
        for _, r in df.iterrows()
    }

def get_ml_model_features(stock_df, nifty_df):
    closes = stock_df["close"].values
    highs = stock_df["high"].values
    lows = stock_df["low"].values
    volumes = stock_df["volume"].values
    nifty_closes = nifty_df["nifty_close"].values
    if len(nifty_closes) < 2 or np.all(np.isnan(nifty_closes)):
        return_total_n = 0.0
        return_mean_n = 0.0
        return_std_n = 0.0
        volatility_n = 0.0
        corr = 0.0
        s_ret = np.diff(closes) / closes[:-1] if len(closes) >= 2 else np.array([0.0])
    else:
        n_ret = np.diff(nifty_closes) / nifty_closes[:-1]
        return_total_n = float((nifty_closes[-1] / nifty_closes[0]) - 1)
        return_mean_n = float(np.mean(n_ret))
        return_std_n = float(np.std(n_ret))
        volatility_n = float(np.std(n_ret))
        # For corr, similar check for s_ret
        s_ret = np.diff(closes) / closes[:-1] if len(closes) >= 2 else np.array([0.0])
        corr = np.corrcoef(s_ret, n_ret)[0, 1] if len(s_ret) == len(n_ret) and len(s_ret) > 0 else 0.0

    def compute_slope(series):
        x = np.arange(len(series)).reshape(-1, 1)
        return LinearRegression().fit(x, series).coef_[0]

    # Max Drawdown logic
    peak = closes[0]
    max_dd = 0.0
    for price in closes:
        if price > peak: peak = price
        max_dd = min(max_dd, (price - peak) / peak)

    return {
        "return_total": float((closes[-1] / closes[0]) - 1),
        "return_mean": float(np.mean(s_ret)),
        "return_std": float(np.std(s_ret)),
        "max_drawdown": float(max_dd),
        "price_slope": float(compute_slope(closes)),
        "volatility_mean": float(np.mean((highs - lows) / (closes + 1e-9))),
        "volume_mean": float(np.mean(volumes)),
        "volume_std": float(np.std(volumes)),
        "volume_trend_slope": float(compute_slope(volumes)),
        "close_position_mean": float(np.mean((closes - lows) / (highs - lows + 1e-9))),
        "nifty_return_total": return_total_n,
        "nifty_return_mean": return_mean_n,
        "nifty_return_std": return_std_n,
        "nifty_volatility": volatility_n,
        "stock_nifty_correlation": float(corr) if not np.isnan(corr) else 0.0
    }

# --- NEW: Sector Analysis Helper ---
def get_sector_performance(symbol, breakout_time, analysis_date):
    """
    Finds peers in the same sector and calculates their average % move 
    from market open till their HIGHEST price reached before breakout_time.
    Formula: ((high - open) / open) * 100
    """
    logger.info(f"üîç Analyzing sector peak performance for {symbol}...")
    
    try:
        # 1. Identify the industry/sector from Supabase
        res = supabase.table("nse_equity_classification") \
            .select("industry") \
            .eq("symbol", symbol) \
            .limit(1) \
            .execute()
        
        if not res.data:
            logger.warning(f"Sector not found for {symbol}")
            return None
        
        target_industry = res.data[0]['industry']
        
        # 2. Fetch peer symbols in that industry
        peer_res = supabase.table("nse_equity_classification") \
            .select("symbol") \
            .eq("industry", target_industry) \
            .execute()
        
        peer_symbols = [row['symbol'] for row in peer_res.data if row['symbol'] != symbol]
        if not peer_symbols:
            return 0.0

        # 3. Calculate peak performance for peers
        moves = []
        start_dt = datetime.datetime.combine(analysis_date, datetime.time(9, 15))
        
        # Limit to 15 peers to maintain speed during live monitoring
        for peer in peer_symbols: 
            token = token_map.get(peer)
            if not token: continue
            
            try:
                # Fetch minute data from open to breakout time
                data = kite.historical_data(token, start_dt, breakout_time, "minute")
                if data:
                    open_p = None
                    for candle in data:
                        ts = candle.get("date")
                        if ts and ts.time() == datetime.time(9, 15):
                            open_p = candle["open"]
                            break

                    if open_p is None:
                        continue  # skip peer completely
                    
                    # Find the HIGHEST price across all candles until breakout time
                    highest_p = max(candle['high'] for candle in data)

                    if open_p <= 0:
                        continue
                    
                    # Calculate percentage move based on peak
                    move = ((highest_p - open_p) / open_p) * 100
                    moves.append(move)
                    time.sleep(0.05)
            except Exception as e:
                continue

        total_peers = len(peer_symbols)
        valid_peers = len(moves)

        if valid_peers == 0:
            logger.warning(f"Sector avg skipped for {symbol}: no valid peer data")
            return None

        coverage_pct = (valid_peers / total_peers) * 100

        if coverage_pct < 60:
            logger.warning(
                f"Sector avg weak for {symbol}: only {valid_peers}/{total_peers} peers ({coverage_pct:.1f}%)"
            )

        avg_peak_move = sum(moves) / valid_peers

        logger.info(
            f"üìä Sector Peak Avg: {avg_peak_move:.2f}% "
            f"(valid {valid_peers}/{total_peers}, coverage {coverage_pct:.1f}%)"
        )
        return round(avg_peak_move, 2)

    except Exception as e:
        logger.error(f"Sector peak check failed: {e}")
        return None

def run_ai_evaluation(breakout, chart_images):
    """Send breakout data to Gemini for YES/NO evaluation."""

    # --- unpack breakout fields ---
    symbol = breakout.get("symbol")
    breakout_price = breakout.get("breakout_price")
    rsi_at_entry = breakout.get("rsi_at_entry")
    percent_rsi_move = breakout.get("percent_rsi_move")
    rsi_at_breakout = breakout.get("rsi_at_breakout")
    volume_multiplier = breakout.get("volume_multiplier")
    volume_spike = breakout.get("volume_spike")
    atr_14 = breakout.get("atr_14")
    avg_daily_vol_20d = breakout.get("avg_daily_vol_20d")
    day_low_price = breakout.get("day_low_price")
    prev_day_high = breakout.get("prev_day_high")
    prev_day_low = breakout.get("prev_day_low")
    pivot_points = breakout.get("pivot_points")
    if not isinstance(pivot_points, dict):
        pivot_points = {"pivot": None, "s1": None, "s2": None, "r1": None, "r2": None}
    nifty_move = breakout.get("nifty_percent_move")
    nifty_above_50ma = breakout.get("nifty_above_50ma")
    breakout_time = breakout.get("breakout_time")
    nifty_value_at_entry = breakout.get("nifty_value_at_entry")
    nifty_value_at_breakout = breakout.get("nifty_value_at_breakout")
    is_bb_squeeze = breakout.get("is_bb_squeeze")
    avg_intraday_range_pct_10d = breakout.get("avg_intraday_range_pct_10d")
    vol_vs_avg_pct_at_breakout = breakout.get("vol_vs_avg_pct_at_breakout")

    # Moving averages (4 daily + 4 hourly)
    ma20_daily  = breakout.get("ma20_daily")
    ma50_daily  = breakout.get("ma50_daily")
    ma100_daily = breakout.get("ma100_daily")
    ma200_daily = breakout.get("ma200_daily")

    ma20_hourly  = breakout.get("ma20_hourly")
    ma50_hourly  = breakout.get("ma50_hourly")
    ma100_hourly = breakout.get("ma100_hourly")
    ma200_hourly = breakout.get("ma200_hourly")

    # --- decode images EXACTLY like your openai version expected ---
    import base64
    img_15m = base64.b64decode(chart_images["img_15m"])
    img_1h  = base64.b64decode(chart_images["img_1h"])
    img_D   = base64.b64decode(chart_images["img_D"])

    latest_news = breakout.get("latest_news")
    if latest_news is None:
        news_context = "No relevant news found for this stock in the last 24 hours."
    else:
        news_context = latest_news

    sector_avg_move = breakout.get("sector_avg_move", "N/A")

    # ---- BUILD PROMPT ----
    ai_prompt = f"""
You are an expert intraday breakout trader with 15+ years experience.

Evaluate the following breakout setup for {symbol} and determine if it is high-probability for a ‚â•3% intraday move.

RSI & MOMENTUM
-----------------------
‚Ä¢ RSI Entry: {rsi_at_entry}
‚Ä¢ RSI Breakout: {rsi_at_breakout}
‚Ä¢ RSI Surge %: {percent_rsi_move}

VOLUME & VOLATILITY
-----------------------
‚Ä¢ Volume Multiplier: {volume_multiplier}
‚Ä¢ Volume Spike: {volume_spike}
‚Ä¢ Vol vs Avg Vol till breakout (%): {vol_vs_avg_pct_at_breakout}
‚Ä¢ ATR(14): {atr_14}
‚Ä¢ Avg Daily Volume (20d): {avg_daily_vol_20d}
‚Ä¢ Breakout Price: {breakout_price}
‚Ä¢ Breakout Time: {breakout_time}

SECTOR & MARKET CONTEXT
-----------------------
‚Ä¢ Sector Average Move (Open to Breakout): {sector_avg_move}%
‚Ä¢ Nifty value at entry: {nifty_value_at_entry}
‚Ä¢ Nifty value at breakout: {nifty_value_at_breakout}
‚Ä¢ Nifty trend (% from entry to breakout): {nifty_move}
‚Ä¢ Nifty above 50MA: {nifty_above_50ma}

PRICE ACTION & LEVELS
-----------------------
‚Ä¢ Day Low: {day_low_price}br
‚Ä¢ Previous Day High: {prev_day_high}
‚Ä¢ Previous Day Low: {prev_day_low}

PIVOTS
-----------------------
‚Ä¢ Pivot: {pivot_points['pivot']}
‚Ä¢ S1: {pivot_points['s1']}
‚Ä¢ S2: {pivot_points['s2']}
‚Ä¢ R1: {pivot_points['r1']}
‚Ä¢ R2: {pivot_points['r2']}

MOVING AVERAGES
-----------------------
‚Ä¢ MA20 Daily: {ma20_daily}
‚Ä¢ MA50 Daily: {ma50_daily}
‚Ä¢ MA100 Daily: {ma100_daily}
‚Ä¢ MA200 Daily: {ma200_daily}
‚Ä¢ MA20 Hourly: {ma20_hourly}
‚Ä¢ MA50 Hourly: {ma50_hourly}
‚Ä¢ MA100 Hourly: {ma100_hourly}
‚Ä¢ MA200 Hourly: {ma200_hourly}

HISTORICAL BEHAVIOUR
-----------------------
‚Ä¢ BB Squeeze active (last 30 sessions): {is_bb_squeeze}
‚Ä¢ Avg intraday range (last 10 days, %): {avg_intraday_range_pct_10d}

LATEST NEWS (Last 24 Hours)
-----------------------
‚Ä¢ {news_context}

PAST 30 TRADING DAYS OHLCV & NIFTY CONTEXT
-----------------------
‚Ä¢{breakout.get('historical_context') or "Historical data not available."}

TASK
-----------------------
Based on all signals above (Technical + News + Chart Pattern + 30-day price action history), do you think the stock will move 2%+ after breakout by the end of the day?

Respond **only** with Yes or No.
"""

    try:
        model = genai.GenerativeModel('gemini-1.5-flash')
        response = model.generate_content(
            contents=[
                ai_prompt,
                {"mime_type": "image/png", "data": img_15m},
                {"mime_type": "image/png", "data": img_1h},
                {"mime_type": "image/png", "data": img_D},
            ]
        )
        ai_answer = response.text.strip()
    except Exception as e:
        logger.error(f"AI eval failed: {e}")
        ai_answer = "AI Unavailable - Proceed Manually"

    # Normalize YES/NO (optional but safe)
    if "yes" in ai_answer.lower():
        ai_answer = "Yes"
    elif "no" in ai_answer.lower():
        ai_answer = "No"

    return ai_answer, ai_prompt

def check_volume_spike(df_subset, lookback=20):
    """Calculate volume spike and return multiplier. Fixed to use proper lookback window."""
    if len(df_subset) <= lookback:
        return False, 1.0
    
    # Use volume data EXCLUDING the current candle for average
    historical_volume = df_subset['volume'].iloc[:-1]  # Exclude last candle
    
    # Calculate average volume over lookback period
    if len(historical_volume) < lookback:
        return False, 1.0  # CHANGED: Default to 1.0x instead of None
    
    avg_vol = historical_volume.tail(lookback).mean()
    current_vol = df_subset['volume'].iloc[-1]
    
    if pd.isna(avg_vol) or avg_vol == 0:
        return False, 1.0  # CHANGED: Default to 1.0x instead of None
    
    multiplier = current_vol / avg_vol
    is_spike = multiplier > 1.5
    
    return is_spike, round(multiplier, 2)
</file>

<file path="past_data_fetch/kite_algo_breakouts.py">
import datetime
import pandas as pd
import numpy as np
import os
from dotenv import load_dotenv
import logging
from supabase import create_client, Client
from decimal import Decimal, ROUND_CEILING
from multiprocessing import cpu_count
import concurrent.futures
import time
from kiteconnect import KiteConnect
from collections import Counter
import subprocess
import sys
import requests
import io

# -------------------------- Logger Setup --------------------------
def setup_logger():
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)
    if logger.handlers:
        for handler in list(logger.handlers):
            logger.removeHandler(handler)
    
    file_handler = logging.FileHandler('kite_algo_breakouts.log', encoding='utf-8', mode='a')
    file_handler.setLevel(logging.INFO)
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    return logger

logger = setup_logger()

# -------------------------- Config & Auth --------------------------
load_dotenv()
SUPABASE_URL = os.getenv("N_SUPABASE_URL")
SUPABASE_KEY = os.getenv("N_SUPABASE_ANON_KEY")
KITE_API_KEY = os.getenv("KITE_API_KEY")
KITE_ACCESS_TOKEN = os.getenv("KITE_ACCESS_TOKEN")

if not all([SUPABASE_URL, SUPABASE_KEY, KITE_API_KEY, KITE_ACCESS_TOKEN]):
    raise ValueError("Missing Supabase or Kite credentials in .env")

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
# Create placeholder for global kite client (do NOT initialize at import-time)
kite = None

def init_kite():
    """Initialize the global Kite client. Call this from main() ‚Äî not at import time."""
    global kite
    if not all([KITE_API_KEY, KITE_ACCESS_TOKEN]):
        raise ValueError("Missing Kite credentials in .env")
    kite = KiteConnect(api_key=KITE_API_KEY)
    kite.set_access_token(KITE_ACCESS_TOKEN)
    try:
        # Validate token once (this will do the network call) ‚Äî keep it inside main()
        kite.profile()
    except Exception as e:
        logger.error(f"‚ùå Kite authentication failed: {e}")
        raise ValueError("Failed to authenticate with Kite. Check your access token.")


# -------------------------- Helper Functions --------------------------

def next_price_above(value, tick):
    """Round up to next valid tick."""
    dv = Decimal(str(value))
    dt = Decimal(str(tick))
    n = (dv / dt).to_integral_value(rounding=ROUND_CEILING)
    candidate = n * dt
    if candidate <= dv:
        candidate = (n + 1) * dt
    return float(candidate)

def get_kite_instruments(nse_symbols):
    """Fetches active NSE stocks (~2200) AND Nifty 50 Index from NSE list."""
    logger.info("Fetching instruments from Kite...")
    
    nse_symbol_set = set(nse_symbols)
    max_retries = 3
    nse_instruments = None

    # Fetch NSE with retry
    for attempt in range(max_retries):
        try:
            if attempt > 0:
                time.sleep(2)
            nse_instruments = kite.instruments("NSE")
            break
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to fetch NSE (Attempt {attempt+1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                logger.error("‚ùå Failed to fetch NSE instruments after all retries.")
                return {}
    
    if not nse_instruments:
        return {}
    
    symbol_map = {}
    nifty_token = None

    # A. Find Nifty 50 Token from NSE list
    for ins in nse_instruments:
        if ins.get('tradingsymbol') == 'NIFTY 50' or ins.get('name') == 'NIFTY 50':
            nifty_token = ins['instrument_token']
            logger.info(f"‚úÖ Found NIFTY 50 token: {nifty_token}")
            break

    # B. Filter Stocks from NSE list
    for ins in nse_instruments:
        symbol = ins["tradingsymbol"]
        if symbol not in nse_symbol_set:
            continue
        symbol_map[symbol] = {
            "token": ins["instrument_token"],
            "tick_size": ins.get("tick_size", 0.05),
            "tradingsymbol": symbol
        }

    logger.info(f"Filtered down to {len(symbol_map)} active NSE equity symbols.")
    
    # C. Add Nifty 50 to the map
    if nifty_token:
        symbol_map["^NSEI"] = {
            "token": nifty_token, 
            "tick_size": 0.05, 
            "tradingsymbol": "NIFTY 50"
        }
    else:
        logger.warning("‚ö†Ô∏è WARNING: NIFTY 50 token not found in NSE list!")
        
    return symbol_map

# -------------------------- Technical Indicators --------------------------
def calculate_rsi(series, period=14):
    delta = series.diff()
    gain = delta.where(delta > 0, 0.0)
    loss = -delta.where(delta < 0, 0.0)
    avg_gain = gain.rolling(window=period, min_periods=period).mean()
    avg_loss = loss.rolling(window=period, min_periods=period).mean()
    rs = avg_gain / avg_loss
    rsi = 100 - (100 / (1 + rs))
    return rsi

def calculate_atr(df, period=14):
    if df.empty or not all(c in df.columns for c in ['high', 'low', 'close']):
        return np.nan
    df = df.copy()
    df['high_low'] = df['high'] - df['low']
    df['high_prev_close'] = np.abs(df['high'] - df['close'].shift(1))
    df['low_prev_close'] = np.abs(df['low'] - df['close'].shift(1))
    tr = df[['high_low', 'high_prev_close', 'low_prev_close']].max(axis=1)
    atr = tr.rolling(window=period, min_periods=period).mean()
    return atr.iloc[-1] if not atr.empty else np.nan

def calculate_pivots(prev_high, prev_low, prev_close):
    if pd.isna(prev_high) or pd.isna(prev_low) or pd.isna(prev_close):
        return {}
    pivot = (prev_high + prev_low + prev_close) / 3
    s1 = (pivot * 2) - prev_high
    r1 = (pivot * 2) - prev_low
    s2 = pivot - (prev_high - prev_low)
    r2 = pivot + (prev_high - prev_low)
    return {
        'pivot': round(pivot, 2),
        's1': round(s1, 2), 'r1': round(r1, 2),
        's2': round(s2, 2), 'r2': round(r2, 2)
    }

# -------------------------- Kite Data Fetching --------------------------

def fetch_history_safe(token, from_date, to_date, interval):
    """
    Wrapper for kite.historical_data with retry logic and error handling.
    Takes in the time period like minute, daily and hourly.
    """
    max_retries = 3
    for attempt in range(max_retries):
        try:
            # Kite historical_data returns list of dicts
            data = kite.historical_data(token, from_date, to_date, interval)
            df = pd.DataFrame(data)
            if not df.empty:
                # Normalize columns to lowercase
                df.rename(columns=str.lower, inplace=True)
                # Ensure date is timezone naive for compatibility with existing logic
                if 'date' in df.columns:
                    df['date'] = pd.to_datetime(df['date'], errors='coerce')
                    df['date'] = df['date'].dt.tz_localize(None)
            return df
        except Exception as e:
            if "Too many requests" in str(e):
                time.sleep(1 + attempt) # Backoff
            elif attempt == max_retries - 1:
                logger.error(f"Failed to fetch {token} data: {e}")
                return pd.DataFrame()
            time.sleep(0.5)
    return pd.DataFrame()

def preload_data_kite(symbols_batch, instrument_map, from_date, to_date):
    """
    Fetches daily and 15m data for a batch of symbols using Kite.
    Respects rate limits (approx 3 req/sec).
    """
    data_dict = {}
    
    # Nifty Token (ensure we have it)
    nifty_meta = instrument_map.get("^NSEI")
    
    # List of symbols to fetch (Batch + Nifty)
    targets = [s for s in symbols_batch if s in instrument_map]
    if nifty_meta and "^NSEI" not in targets:
        targets.insert(0, "^NSEI")

    for symbol in targets:
        meta = instrument_map[symbol]
        token = meta['token']
        
        # 1. Fetch Daily Data
        df_daily = fetch_history_safe(token, from_date, to_date, "day")
        if df_daily.empty:
            continue
            
        # 2. Fetch Intraday (15minute) - Kite allows max ~100 days for 15m.
        intraday_start = max(from_date, to_date - datetime.timedelta(days=60))
        df_minute = fetch_history_safe(token, intraday_start, to_date, "15minute")
        
        if df_minute.empty:
            continue

        # Create Hourly aggregate from minute data for Mas
        df_minute_sorted = df_minute.sort_values('date').drop_duplicates('date').reset_index(drop=True)
        df_hourly = df_minute_sorted.set_index('date').resample('60min').agg({
            'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last', 'volume': 'sum'
        }).dropna().reset_index()

        key = 'nifty' if symbol == "^NSEI" else symbol
        data_dict[key] = {
            'daily': df_daily,
            'minute': df_minute_sorted,
            'hourly': df_hourly,
            'tick_size': meta['tick_size']
        }
        
        time.sleep(0.4) 

    return data_dict

# -------------------------- MA Precompute --------------------------
def precompute_mas(data_dict, symbols):
    ma_dict = {}
    
    # Handle Nifty separately if needed, or include in loop
    keys = list(symbols)
    if 'nifty' in data_dict:
        keys.append('nifty')

    for symbol in keys:
        if symbol not in data_dict:
            continue
        ma_dict[symbol] = {}
        for interval in ['daily', 'hourly']:
            df = data_dict[symbol].get(interval)
            if df is None or df.empty or 'close' not in df.columns:
                continue
            ma_df = pd.DataFrame({'date': df['date'].copy()})
            close = df['close']
            for p in [20, 50, 100, 200]:
                if len(close) >= p:
                    # Shift(1) because we use yesterday's MA to judge today's entry
                    ma_df[f'ma{p}'] = close.rolling(p).mean().shift(1)
                else:
                    ma_df[f'ma{p}'] = np.nan
            ma_dict[symbol][interval] = ma_df
    return ma_dict

# -------------------------- Logic Components --------------------------
def get_ma_thresholds(ma_dict, symbol, analysis_date):
    daily = ma_dict.get(symbol, {}).get('daily')
    hourly = ma_dict.get(symbol, {}).get('hourly')
    if daily is None or daily.empty:
        return 'no_data', None

    # Date comparison: ensure both are dates
    day_row = daily[daily['date'].dt.date == analysis_date]
    if day_row.empty:
        return 'no_data', None
    
    day_mas = [day_row.iloc[0][f'ma{p}'] for p in [20,50,100,200]]

    # For hourly, we take the last closed candle *before* the analysis date starts
    if hourly is not None and not hourly.empty:
        prev_hourly = hourly[hourly['date'] < pd.Timestamp(analysis_date)]
        if not prev_hourly.empty:
            last = prev_hourly.iloc[-1]
            day_mas.extend([last[f'ma{p}'] for p in [20,50,100,200]])

    if any(pd.isna(x) for x in day_mas):
        return 'nan', None
    return 'success', (min(day_mas), max(day_mas))

def check_volume_spike(df_subset, lookback=20):
    if len(df_subset) <= lookback:
        return False, 1.0
    
    historical_volume = df_subset['volume'].iloc[:-1]
    if len(historical_volume) < lookback:
        return False, 1.0
    
    avg_vol = historical_volume.tail(lookback).mean()
    current_vol = df_subset['volume'].iloc[-1]
    
    if pd.isna(avg_vol) or avg_vol == 0:
        return False, 1.0
    
    multiplier = current_vol / avg_vol
    is_spike = multiplier > 1.5
    return is_spike, round(multiplier, 2)

def calculate_bb_squeeze(df, period=20, std_dev=2, kc_period=20, kc_atr_mult=1.5):
    """
    Detects Bollinger Band Squeeze.
    Returns True if BB width is narrower than Keltner Channel width.
    """
    if len(df) < period:
        return False
    
    # Bollinger Bands
    sma = df['close'].rolling(period).mean()
    std = df['close'].rolling(period).std()
    bb_upper = sma + (std_dev * std)
    bb_lower = sma - (std_dev * std)
    bb_width = bb_upper - bb_lower
    
    # Keltner Channels
    kc_middle = df['close'].rolling(kc_period).mean()
    atr = calculate_atr(df, kc_period)
    kc_upper = kc_middle.iloc[-1] + (kc_atr_mult * atr) if pd.notna(atr) else np.nan
    kc_lower = kc_middle.iloc[-1] - (kc_atr_mult * atr) if pd.notna(atr) else np.nan
    kc_width = kc_upper - kc_lower

    # Squeeze occurs when BB is inside KC
    if pd.notna(bb_width.iloc[-1]) and pd.notna(kc_width):
        return bb_width.iloc[-1] < kc_width
    return False

# -------------------------- Core Processing --------------------------
def process_stock_day(task):
    symbol = task['symbol']
    date = task['analysis_date']
    intraday = task['intraday_data']
    daily = task['daily_data']
    hourly = task['hourly_data'] # Ensure this is passed in main()
    tick_size = task['tick_size']
    nifty_intraday = task.get('nifty_intraday')
    nifty_50ma = task.get('nifty_50ma')

    if intraday.empty or daily.empty or hourly is None or hourly.empty:
        return ('no_data', symbol)

    # --- 1. GET TODAY'S OPEN ---
    df_today = intraday[intraday['date'].dt.date == date].sort_values('date').reset_index(drop=True)
    if df_today.empty: return ('no_intraday_today', symbol)
    day_open = float(df_today.iloc[0]['open'])

    # --- 2. RE-CALCULATE MAs (History + Day Open) ---
    
    # A. Daily MAs
    daily_hist = daily[daily['date'].dt.date < date].sort_values('date').reset_index(drop=True)
    if len(daily_hist) < 200: return ('not_enough_history', symbol)
    
    daily_prices = list(daily_hist['close'].values)
    daily_prices.append(day_open) 
    daily_series = pd.Series(daily_prices)
    
    daily_mas = []
    for p in [20, 50, 100, 200]:
        val = daily_series.rolling(window=p).mean().iloc[-1]
        daily_mas.append(float(val))

    # B. Hourly MAs
    hourly_hist = hourly[hourly['date'] < pd.Timestamp(date)].sort_values('date').reset_index(drop=True)
    if len(hourly_hist) < 200: return ('not_enough_history', symbol)
    
    hourly_prices = list(hourly_hist['close'].values)
    hourly_prices.append(day_open)
    hourly_series = pd.Series(hourly_prices)
    
    hourly_mas = []
    for p in [20, 50, 100, 200]:
        val = hourly_series.rolling(window=p).mean().iloc[-1]
        hourly_mas.append(float(val))
        
    # --- 3. DETERMINE THRESHOLDS ---
    mas_list = daily_mas + hourly_mas
    if any(pd.isna(x) for x in mas_list): return ('nan_mas', symbol)
    
    # In this script, we typically look for the Min MA (Dip) and Max MA (Breakout)
    min_ma = min(mas_list)
    max_ma = max(mas_list)

    # --- 4. MONITOR CHECK ---
    # Stock must start below the Lowest MA
    if day_open < min_ma:
        entry_time = df_today.iloc[0]['date'] - pd.Timedelta(seconds=1)
    else:
        return ('no_dip', symbol)

    # --- 5. BREAKOUT LOGIC ---
    post_dip = df_today[df_today['date'] > entry_time].copy()

    # Calculate the precise breakout price (1 tick above max_ma)
    breakout_threshold = next_price_above(max_ma, tick_size)

    # Check if either CLOSE or OPEN crossed the threshold
    post_dip['breakout'] = (post_dip['close'] >= breakout_threshold) | (post_dip['open'] >= breakout_threshold)

    breakout_candles = post_dip[post_dip['breakout']]
    if breakout_candles.empty:
        return ('no_breakout', symbol)    
    
    first_breakout = breakout_candles.iloc[0]
    breakout_time = first_breakout['date']

    # --- 6. PRECISE PRICE CALCULATION ---
    breakout_threshold = next_price_above(max_ma, tick_size)
    candle_open = float(first_breakout['open'])
    breakout_price = max(candle_open, breakout_threshold)
    
    # --- 7. METRICS ---
    # Calculate Day High after breakout
    post_breakout_data = df_today[df_today['date'] >= breakout_time]
    day_high = post_breakout_data['high'].max() if not post_breakout_data.empty else breakout_price
    day_low = df_today['low'].min()
    
    # Indicators
    atr_14 = calculate_atr(daily_hist.tail(30), period=14)
    is_bb_squeeze = bool(calculate_bb_squeeze(daily_hist.tail(50)))
    
    # RSI
    hist_to_entry = intraday[intraday['date'] <= entry_time]
    rsi_entry = calculate_rsi(hist_to_entry['close']).iloc[-1] if len(hist_to_entry) >= 14 else np.nan
    
    hist_to_breakout = intraday[intraday['date'] <= breakout_time]
    rsi_breakout = calculate_rsi(hist_to_breakout['close']).iloc[-1] if len(hist_to_breakout) >= 14 else np.nan
    
    percent_rsi_move = 0.0
    if pd.notna(rsi_entry) and pd.notna(rsi_breakout) and rsi_entry != 0:
        percent_rsi_move = ((rsi_breakout - rsi_entry) / rsi_entry) * 100

    # Nifty Metrics
    nifty_at_entry = None
    nifty_at_breakout = None
    nifty_move = 0.0
    nifty_is_above_val = False

    if nifty_intraday is not None and not nifty_intraday.empty:
        # Nifty Entry
        n_entry_rows = nifty_intraday[nifty_intraday.index <= entry_time]
        if not n_entry_rows.empty:
            nifty_at_entry = float(n_entry_rows.iloc[-1]['close'])
        else:
            nifty_at_entry = float(nifty_intraday.iloc[0]['open'])
            
        # Nifty Breakout
        n_break_rows = nifty_intraday[nifty_intraday.index <= breakout_time]
        if not n_break_rows.empty:
            nifty_at_breakout = float(n_break_rows.iloc[-1]['close'])
        
        # Calculation
        if nifty_at_entry and nifty_at_breakout and nifty_at_entry != 0:
            nifty_move = ((nifty_at_breakout - nifty_at_entry) / nifty_at_entry) * 100
            
        if nifty_at_breakout and nifty_50ma:
            nifty_is_above_val = nifty_at_breakout > nifty_50ma

    # Volume
    spike, vol_mult = check_volume_spike(hist_to_breakout)
    avg_vol_20 = daily_hist['volume'].tail(20).mean()
    avg_daily_vol_20d = float(avg_vol_20) if pd.notna(avg_vol_20) else None
    
    vol_at_breakout = df_today[df_today['date'] <= breakout_time]['volume'].sum()
    vol_vs_avg_pct = (vol_at_breakout / avg_daily_vol_20d * 100) if avg_daily_vol_20d and avg_daily_vol_20d > 0 else 0.0
    
    # Previous Day & Pivots
    prev_day = daily_hist.iloc[-1]
    pivots = calculate_pivots(prev_day['high'], prev_day['low'], prev_day['close'])
    
    df_hist_10 = daily_hist.tail(10).copy()
    df_hist_10['range_pct'] = ((df_hist_10['high'] - df_hist_10['low']) / df_hist_10['low']) * 100
    avg_intraday_range_pct = df_hist_10['range_pct'].mean()

    # Store MAs for reference
    mas_dict = {}
    periods = [20, 50, 100, 200]
    for i, val in enumerate(daily_mas): mas_dict[f'daily_ma{periods[i]}'] = val
    for i, val in enumerate(hourly_mas): mas_dict[f'hourly_ma{periods[i]}'] = val

    return 'success', {
        'symbol': symbol,
        'breakout_date': date.strftime('%Y-%m-%d'),
        'monitor_entry_time': entry_time.isoformat(),
        'breakout_time': breakout_time.isoformat(),
        'breakout_price': float(breakout_price),
        'min_ma': float(min_ma),
        'max_ma': float(max_ma),
        'day_high_price': float(day_high),
        'day_low_price': float(day_low),
        'percent_move': round(((day_high - breakout_price) / breakout_price) * 100, 2),
        'rsi_at_entry': float(rsi_entry) if pd.notna(rsi_entry) else None,
        'rsi_at_breakout': float(rsi_breakout) if pd.notna(rsi_breakout) else None,
        'percent_rsi_move': float(percent_rsi_move),
        'nifty_percent_move': float(nifty_move),
        'nifty_value_at_entry': float(nifty_at_entry) if nifty_at_entry else None,
        'nifty_value_at_breakout': float(nifty_at_breakout) if nifty_at_breakout else None,
        'nifty_above_50ma': bool(nifty_is_above_val), 
        'volume_spike': bool(spike),
        'is_bb_squeeze': bool(is_bb_squeeze),
        'volume_multiplier': float(vol_mult),
        **mas_dict,
        'atr_14': float(atr_14) if pd.notna(atr_14) else None,
        'prev_day_high': float(prev_day['high']),
        'prev_day_low': float(prev_day['low']),
        'avg_intraday_range_pct_10d': float(avg_intraday_range_pct),
        'vol_vs_avg_pct_at_breakout': float(vol_vs_avg_pct),
        'pivot_points': pivots,
        'avg_daily_vol_20d': avg_daily_vol_20d
    }

# -------------------------- Main Pipeline --------------------------

def trigger_screenshot_scraper(symbol, breakout_date):
    """
    Calls the screenshot_scrapper_2.py script for a specific stock.
    Converts 'RELIANCE.NS' -> 'NSE:RELIANCE' for TradingView compatibility.
    """
    try:
        # Convert Yahoo format (RELIANCE.NS) to TradingView format (NSE:RELIANCE)
        tv_symbol = f"NSE:{symbol.replace('.NS', '')}"
        
        logger.info(f"üì∏ Taking screenshots for {tv_symbol}...")
        
        # We use subprocess.run (blocking) to prevent opening 50 browsers at once and crashing RAM.
        # If you want it to run in background, change .run() to .Popen()
        subprocess.run([
            sys.executable, "screenshot_scrapper_2.py",
            "--symbol", tv_symbol,
            "--dt", breakout_date,
            "--out", "./breakout_ss",  # Images will be saved here
            "--headless"                   # Run without opening visible browser windows
        ], check=True, timeout=120)
        
    except Exception as e:
        logger.error(f"‚ùå Screenshot failed for {symbol}: {e}")

def save_breakouts_to_excel(data):
    try:
        new_df = pd.DataFrame(data)

        if os.path.exists("custom_ma_breakouts_backup.xlsx"):
            old_df = pd.read_excel("custom_ma_breakouts_backup.xlsx")
            combined = pd.concat([old_df, new_df], ignore_index=True)
        else:
            combined = new_df

        # Deduplicate by symbol + breakout_date
        combined = combined.drop_duplicates(
            subset=["symbol", "breakout_date"],
            keep="last"
        )

        combined.to_excel("custom_ma_breakouts_backup.xlsx", index=False)

        logger.info("üìÅ Saved Excel fallback: custom_ma_breakouts_backup.xlsx")

    except Exception as e:
        logger.error(f"‚ùå Failed writing Excel backup: {e}")

def batch_upsert_supabase(data):
    if not data:
        return

    unique_data = list({ (d['symbol'], d['breakout_date']): d for d in data }.values())

    try:
        supabase.table("algo_version2_breakouts") \
            .upsert(unique_data, on_conflict='symbol,breakout_date') \
            .execute()

        logger.info(f"Upserted {len(unique_data)} records.")

    except Exception as e:
        logger.error(f"Supabase upsert error: {e}")
        save_breakouts_to_excel(unique_data)

def get_all_nse_symbols():
    """Fetch all active NSE equity symbols from NSE (EQ series only)."""
    logger.info("Fetching NSE symbol list from NSE CSV...")
    
    url = "https://archives.nseindia.com/content/equities/EQUITY_L.csv"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
    }
    
    max_retries = 3
    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            df = pd.read_csv(io.StringIO(response.text))
            break
        except Exception as e:
            if attempt < max_retries - 1:
                logger.warning(f"Retry {attempt+1}/{max_retries} failed to fetch NSE CSV: {e}")
                time.sleep(5)
            else:
                logger.error("Failed to fetch NSE symbol list.")
                raise

    df.columns = df.columns.str.strip()
    df = df[df["SERIES"] == "EQ"]
    symbols = df["SYMBOL"].tolist()

    logger.info(f"Fetched {len(symbols)} NSE EQ symbols from NSE.")
    return symbols

def main():
    logger.info("=== KITE BREAKOUT SCANNER (DEBUG MODE) ===")

    init_kite()   # initialize Kite client here (safe ‚Äî happens only in the main process)    
    nse_symbols = get_all_nse_symbols()        # ~2065 symbols
    instrument_map = get_kite_instruments(nse_symbols)

    # Only symbols that Kite actually supports
    all_symbols = [s for s in nse_symbols if s in instrument_map]
    logger.info(f"NSE symbols: {len(nse_symbols)} | Kite-mapped symbols: {len(all_symbols)}")

    target_date = datetime.date(2026, 2, 6)
    target_dates = [target_date]
    logger.info(f"Targeting dates: {target_dates}")

    fetch_start = target_date - datetime.timedelta(days=400)
    fetch_end = target_date
    
    batch_size = 50
    
    for i in range(0, len(all_symbols), batch_size):
        batch_symbols = all_symbols[i:i+batch_size]
        logger.info(f"--- Processing Batch {i//batch_size + 1} ({len(batch_symbols)} symbols) ---")
        
        data_dict = preload_data_kite(batch_symbols, instrument_map, fetch_start, fetch_end)
        
        # --- DEBUG CHECK: Are we actually getting data? ---
        if not data_dict:
            logger.error("CRITICAL: Data Dictionary is empty! Check API Keys or Internet.")
            continue
        
        if 'nifty' not in data_dict:
            logger.warning("Nifty data fetch failed, skipping batch.")
            continue
            
        
        nifty_daily = data_dict['nifty']['daily']
        nifty_daily['ma50'] = nifty_daily['close'].rolling(50).mean().shift(1)
        nifty_50ma_lookup = dict(zip(nifty_daily['date'].dt.date, nifty_daily['ma50']))
        nifty_intraday_full = data_dict['nifty']['minute']
        
        tasks = []
        for sym in batch_symbols:
            if sym not in data_dict:
                continue
            
            for d in target_dates:
                daily_dates = data_dict[sym]['daily']['date'].dt.date.values
                if d not in daily_dates:
                    continue

                task = {
                    'symbol': sym,
                    'analysis_date': d,
                    'intraday_data': data_dict[sym]['minute'],
                    'daily_data': data_dict[sym]['daily'],
                    'hourly_data': data_dict[sym]['hourly'],
                    'nifty_intraday': nifty_intraday_full[nifty_intraday_full['date'].dt.date == d].set_index('date'),
                    'nifty_50ma': nifty_50ma_lookup.get(d),
                    'tick_size': data_dict[sym]['tick_size']
                }
                tasks.append(task)
        
        results = []
        failure_reasons = Counter() # Track why stocks fail

        if tasks:
            with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count()) as executor:
                futures = {executor.submit(process_stock_day, t): t for t in tasks}
                for f in concurrent.futures.as_completed(futures):
                    status, res = f.result()
                    if status == 'success':
                        results.append(res)
                    else:
                        failure_reasons[status] += 1 # Count the failure reason
        
        # --- DEBUG LOGGING ---
        if results:
            logger.info(f"‚úÖ Found {len(results)} breakouts!")
            
            cleaned_results = []
            for r in results:
                # 1. Trigger the Screenshot Scraper
                trigger_screenshot_scraper(r['symbol'], r['breakout_time'])
                
                # 2. Clean up data for DB
                cleaned_results.append(r)
            
            # 3. Upload to Supabase
            batch_upsert_supabase(cleaned_results)
        else:
            logger.info("‚ùå No breakouts.")
            # PRINT THE REASONS WHY
            logger.info(f"   [Rejection Stats]: {dict(failure_reasons)}")
            
        time.sleep(1)

    logger.info("=== SCAN COMPLETE ===")

if __name__ == "__main__":
    main()
</file>

<file path="past_data_fetch/paper_trading.py">
import datetime
import pandas as pd
import numpy as np
import os
import logging
import io
import requests
from kiteconnect import KiteConnect
from dotenv import load_dotenv
from decimal import Decimal, ROUND_CEILING
import concurrent.futures

# -------------------------- Setup & Config --------------------------
load_dotenv()
KITE_API_KEY = os.getenv("KITE_API_KEY")
KITE_ACCESS_TOKEN = os.getenv("KITE_ACCESS_TOKEN")

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Initialize Kite
kite = KiteConnect(api_key=KITE_API_KEY)
kite.set_access_token(KITE_ACCESS_TOKEN)

# -------------------------- Helper Functions --------------------------

def next_price_above(value, tick):
    """Calculates the exact entry price 1 tick above the MA."""
    dv = Decimal(str(value))
    dt = Decimal(str(tick))
    n = (dv / dt).to_integral_value(rounding=ROUND_CEILING)
    candidate = n * dt
    if candidate <= dv:
        candidate = (n + 1) * dt
    return float(candidate)

def calculate_rsi(series, period=14):
    delta = series.diff()
    gain = (delta.where(delta > 0, 0)).ewm(com=period-1, adjust=False).mean()
    loss = (-delta.where(delta < 0, 0)).ewm(com=period-1, adjust=False).mean()
    rs = gain / loss
    return 100 - (100 / (1 + rs))

def calculate_pivots(prev_high, prev_low, prev_close):
    pivot = (prev_high + prev_low + prev_close) / 3
    return {
        'pivot': round(pivot, 2),
        's1': round((pivot * 2) - prev_high, 2),
        'r1': round((pivot * 2) - prev_low, 2)
    }

def calculate_atr(df, period=14):
    if df.empty or not all(c in df.columns for c in ['high', 'low', 'close']):
        return np.nan
    df = df.copy()
    df['high_low'] = df['high'] - df['low']
    df['high_prev_close'] = np.abs(df['high'] - df['close'].shift(1))
    df['low_prev_close'] = np.abs(df['low'] - df['close'].shift(1))
    tr = df[['high_low', 'high_prev_close', 'low_prev_close']].max(axis=1)
    return tr.rolling(window=period).mean().iloc[-1]

def calculate_bb_squeeze(df, period=20, std_dev=2, kc_atr_mult=1.5):
    sma = df['close'].rolling(period).mean()
    std = df['close'].rolling(period).std()
    bb_width = (sma + (std_dev * std)) - (sma - (std_dev * std))
    atr = calculate_atr(df, period)
    if pd.isna(atr) or pd.isna(sma.iloc[-1]) or pd.isna(bb_width.iloc[-1]):
        return False

    kc_width = (sma.iloc[-1] + (kc_atr_mult * atr)) - (sma.iloc[-1] - (kc_atr_mult * atr))

    return bb_width.iloc[-1] < kc_width


def get_active_nse_symbols():
    """Fetches the list of EQ series stocks from NSE and cleans column names."""
    url = "https://archives.nseindia.com/content/equities/EQUITY_L.csv"
    headers = {'User-Agent': 'Mozilla/5.0'}
    try:
        res = requests.get(url, headers=headers)
        df = pd.read_csv(io.StringIO(res.text))
        
        df.columns = df.columns.str.strip()
        
        return df[df["SERIES"] == "EQ"]["SYMBOL"].tolist()
    except Exception as e:
        logger.error(f"Failed to fetch NSE symbols: {e}")
        return []

def fetch_history(token, start, end, interval):
    cols = ['date', 'open', 'high', 'low', 'close', 'volume']
    try:
        data = kite.historical_data(token, start, end, interval)
        df = pd.DataFrame(data)
        if not df.empty:
            df.rename(columns=str.lower, inplace=True)
            df['date'] = pd.to_datetime(df['date']).dt.tz_localize(None)
            return df
        return pd.DataFrame(columns=cols) # Return empty with columns
    except Exception as e:
        logger.error(f"Fetch error: {e}")
        return pd.DataFrame(columns=cols) # Return empty with columns

# -------------------------- Trade Logic --------------------------

def process_paper_trade(df_today, breakout_price, breakout_time):
    """
    Standard Exit Logic: 
    - SL: -1% | Target: +3% | Time: 3:15 PM EOD
    """

    df_today = df_today.copy()
    df_today['ma8'] = df_today['close'].rolling(window=8).mean()
    
    trade_data = df_today[df_today['date'] >= breakout_time].sort_values('date')
    if trade_data.empty:
        return df_today.iloc[-1]['close'], "Late Entry/No Data", 0, 0

    target_price = breakout_price * 1.03
    sl_price = breakout_price
    
    is_in_trade = True
    total_pnl_pct = 0
    exit_reason = "Market Close"
    last_exit_price = breakout_price

    for idx, row in trade_data.iterrows():
        # Current candle data
        high, low, close, ma8 = row['high'], row['low'], row['close'], row['ma8']
        
        if is_in_trade:
            # CHECK EXIT: Hit Stop Loss
            if low <= sl_price:
                is_in_trade = False
                last_exit_price = sl_price
                exit_reason = "Stopped out @ Breakout"
                # PnL is 0 because we exit at entry price
            
            # CHECK EXIT: Hit Target
            elif high >= target_price:
                return target_price, "Target hit @3%", 3, 0
        
        else:
            # NEW RE-ENTRY: Buy again if price crosses back ABOVE the breakout price
            if close > breakout_price:
                is_in_trade = True
                exit_reason = "Re-entered above Breakout"

        # FINAL EXIT: EOD 3:15 PM
        if row['date'].time() >= datetime.time(15, 15):
            if is_in_trade:
                last_exit_price = close
                pnl = ((last_exit_price - breakout_price) / breakout_price) * 100
                return last_exit_price, "EOD Exit @15:15", max(0, round(pnl, 2)), max(0, round(-pnl, 2))
            else:
                return last_exit_price, exit_reason, 0, 0

    return last_exit_price, exit_reason, 0, 0

# -------------------------- Main Scanner --------------------------

def process_symbol_for_date(sym, token, current_date, df_daily, df_15m, n_df_intra, nifty_50ma, rsi_series, df_hourly_full):
    results = []

    # Safety Check: Skip if data is invalid or missing columns
    if df_daily is None or df_daily.empty or 'date' not in df_daily.columns:
        return results
    if df_15m is None or df_15m.empty or 'date' not in df_15m.columns:
        return results

    df_today = df_15m[df_15m['date'].dt.date == current_date]
    if df_today.empty:
        return results

    day_open = float(df_today.iloc[0]['open'])
    prev_daily = df_daily[df_daily['date'].dt.date < current_date].tail(199)

    if len(prev_daily) < 199:
        return results

    df_hourly = df_hourly_full[df_hourly_full['date'].dt.date < current_date].tail(200)

    if len(df_hourly) < 200:
        return results

    d_prices = list(prev_daily['close'].values) + [day_open]
    h_prices = list(df_hourly['close'].values) + [day_open]

    # Using simple list slicing/summing is faster than creating 8 new pandas Series objects per symbol
    def get_ma(prices, period):
        return sum(prices[-period:]) / period if len(prices) >= period else 0

    daily_mas = [get_ma(d_prices, p) for p in [20, 50, 100, 200]]
    hourly_mas = [get_ma(h_prices, p) for p in [20, 50, 100, 200]]

    min_ma, max_ma = min(daily_mas + hourly_mas), max(daily_mas + hourly_mas)

    if not (day_open < min_ma and df_today['high'].max() > max_ma):
        return results

    breakout = df_today[
        ((df_today['close'] >= next_price_above(max_ma, 0.05)) |
        (df_today['open'] >= next_price_above(max_ma, 0.05))) &
        (df_today['date'].dt.time <= datetime.time(15, 10))
    ]

    if breakout.empty:
        return results

    first = breakout.iloc[0]
    b_time = first['date']
    b_price = max(first['open'], next_price_above(max_ma, 0.05))

    # Nifty
    nifty_entry = n_df_intra.iloc[0]['open']
    nifty_break = n_df_intra[n_df_intra.index <= b_time].iloc[-1]['close']

    # ---------- RSI ----------
    entry_time = df_today.iloc[0]['date']

    rsi_entry = rsi_series[df_15m['date'] <= entry_time].dropna().iloc[-1]
    rsi_break = rsi_series[df_15m['date'] <= b_time].dropna().iloc[-1]

    # ---------- VOLUME ----------
    avg_vol_20d = prev_daily['volume'].mean()
    vol_at_break = df_today[df_today['date'] <= b_time]['volume'].sum()
    vol_mult = round(vol_at_break / avg_vol_20d, 2) if avg_vol_20d > 0 else 0

    # ---------- ATR ----------
    atr_val = calculate_atr(prev_daily.tail(30))

    # ---------- BB SQUEEZE ----------
    is_squeeze = calculate_bb_squeeze(prev_daily.tail(50))

    # ---------- INTRADAY RANGE ----------
    df_hist_10 = prev_daily.tail(10).copy()
    df_hist_10['range_pct'] = ((df_hist_10['high'] - df_hist_10['low']) / df_hist_10['low']) * 100
    avg_range_10d = df_hist_10['range_pct'].mean()

    # ---------- PIVOTS ----------
    prev_day = prev_daily.iloc[-1]
    pivots = calculate_pivots(prev_day['high'], prev_day['low'], prev_day['close'])

    results.append({
        'Symbol': sym,
        'Breakout Date': current_date,
        'Time': b_time.time(),
        'Breakout Price': b_price,
        'df_today': df_today,
        'monitor_entry_time': df_today.iloc[0]['date'],
        'rsi_entry': rsi_entry,
        'rsi_break': rsi_break,
        'nifty_entry': nifty_entry,
        'nifty_break': nifty_break,
        'nifty_50ma': nifty_50ma,
        'atr_val': atr_val,
        'is_squeeze': is_squeeze,
        'vol_mult': vol_mult,
        'avg_range_10d': avg_range_10d,
        'avg_vol': avg_vol_20d,
        'vol_at_break': vol_at_break,
        'pivots': pivots,
        'prev_day': prev_day,
        'min_ma': min_ma,
        'max_ma': max_ma,
        'daily_mas': daily_mas,
        'hourly_mas': hourly_mas
    })
    return results

def run_scanner():
    daily_cache = {}
    intraday_cache = {}
    rsi_cache = {}
    hourly_cache = {}
    
    target_dates = pd.date_range(start="2026-02-16", end="2026-02-19").date               # FOR A WEEK

    # target_dates = pd.date_range(start="2026-02-02", end="2026-02-02").date             # FOR TODAY

    # Generates a list of dates for the last 30 days
    # target_dates = pd.date_range(end=datetime.date.today(), periods=30).date

    symbols = get_active_nse_symbols()
    
    inst = kite.instruments("NSE")
    inst_map = {i['tradingsymbol']: i['instrument_token'] for i in inst if i['tradingsymbol'] in symbols}

    import pickle
    cache_file = "market_data_cache.pkl"

    if os.path.exists(cache_file):
        logger.info(">>> SUCCESS: Found saved data on disk. Loading now...")
        with open(cache_file, "rb") as f:
            loaded_data = pickle.load(f)
            daily_cache = loaded_data['daily']
            intraday_cache = loaded_data['intraday']
            rsi_cache = loaded_data['rsi']
            hourly_cache = loaded_data['hourly']
        logger.info(f">>> LOADED: {len(daily_cache)} symbols ready for analysis.")
    else:
        logger.info(">>> STARTING PRELOAD: This will take ~15 mins. Logs will appear every 50 symbols.")
        
        def preload_worker(sym, token):
            d = fetch_history(token, min(target_dates) - datetime.timedelta(days=400), max(target_dates), "day")
            i = fetch_history(token, min(target_dates) - datetime.timedelta(days=100), max(target_dates), "15minute")
            rsi = calculate_rsi(i['close']) if not i.empty else pd.Series()
            h_full = i.set_index('date').resample('60min').agg({'close': 'last'}).dropna().reset_index() if not i.empty else pd.DataFrame()
            return token, d, i, rsi, h_full

        completed = 0
        total = len(inst_map)

        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            future_to_sym = {executor.submit(preload_worker, s, t): t for s, t in inst_map.items()}
            for future in concurrent.futures.as_completed(future_to_sym):
                try:
                    t, d, i, rsi, h_full = future.result()
                    daily_cache[t], intraday_cache[t] = d, i
                    rsi_cache[t], hourly_cache[t] = rsi, h_full
                    
                    completed += 1
                    if completed % 50 == 0: # Log every 50 so you see more updates
                        logger.info(f"Preload Progress: {completed}/{total} ({(completed/total)*100:.1f}%)")
                except Exception as e:
                    logger.error(f"Worker Error: {e}")

        # SAVE immediately after preloading so you never have to do this again
        cache_data = {'daily': daily_cache, 'intraday': intraday_cache, 'rsi': rsi_cache, 'hourly': hourly_cache}
        with open(cache_file, "wb") as f:
            pickle.dump(cache_data, f)
        logger.info(f">>> SAVED: Data stored to {cache_file}. Next run will be instant.")

    # --- REST OF THE SCRIPT (Nifty Fetching & Scanner) ---
    nifty_token = 256265 
    n_daily_full = fetch_history(nifty_token, min(target_dates) - datetime.timedelta(days=100), max(target_dates), "day")
    n_daily_full['ma50'] = n_daily_full['close'].rolling(50).mean().shift(1)
    n_intra_full = fetch_history(nifty_token, min(target_dates), max(target_dates), "15minute")

    final_results = []
    daily_total_budget = 20000

    for current_date in target_dates:
        logger.info(f"--- Processing Date: {current_date} ---")
        n_df_intra = n_intra_full[n_intra_full['date'].dt.date == current_date].set_index('date')
        nifty_row = n_daily_full[n_daily_full['date'].dt.date == current_date]
        
        if nifty_row.empty or n_df_intra.empty: continue
        nifty_50ma = nifty_row['ma50'].iloc[0]
        day_breakouts = []

        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            futures = []
            for sym, token in inst_map.items():
                if token not in intraday_cache or intraday_cache[token].empty or 'date' not in daily_cache[token].columns:
                    continue
                futures.append(executor.submit(process_symbol_for_date, sym, token, current_date, daily_cache[token], intraday_cache[token], n_df_intra, nifty_50ma, rsi_cache[token], hourly_cache[token]))

            for f in concurrent.futures.as_completed(futures):
                day_breakouts.extend(f.result())
            logger.info(f"Finished {current_date}: Found {len(day_breakouts)} breakout trades.")

        if day_breakouts:
            amt_per_trade = daily_total_budget / len(day_breakouts)
            for trade in day_breakouts:
                exit_p, reason, p_pct, l_pct = process_paper_trade(trade['df_today'], trade['Breakout Price'], pd.Timestamp.combine(trade['Breakout Date'], trade['Time']))
                df_t = trade.get('df_today')
                prev_d = trade.get('prev_day')

                final_results.append({
                    'Symbol': trade.get('Symbol'), 
                    'Breakout Date': pd.Timestamp.combine(trade.get('Breakout Date'), trade.get('Time')),
                    'Time': trade.get('Time'),
                    'monitor_entry_time': trade.get('monitor_entry_time'),
                    'Breakout Price': trade.get('Breakout Price'),
                    'High Price': df_t['high'].max() if (df_t is not None and not df_t.empty) else 0,
                    'Selling Price': exit_p, 
                    'Selling Price condition': reason,
                    'Trade Amount': amt_per_trade, 
                    'Profit %': p_pct,
                    'Loss %': l_pct,
                    'Day %Move': round(((df_t['high'].max() - trade.get('Breakout Price')) / trade.get('Breakout Price')) * 100, 2) if (df_t is not None and not df_t.empty) else 0,
                    'rsi_at_entry': trade.get('rsi_entry'),
                    'rsi_at_breakout': trade.get('rsi_break'),
                    'rsi_percent_move': ((trade.get('rsi_break') - trade.get('rsi_entry')) / trade.get('rsi_entry')) * 100 if trade.get('rsi_entry', 0) != 0 else 0,
                    'nifty_at_entry': trade.get('nifty_entry'),
                    'nifty_at_breakout': trade.get('nifty_break'),
                    'nifty_percent_move': ((trade.get('nifty_break') - trade.get('nifty_entry')) / trade.get('nifty_entry')) * 100 if trade.get('nifty_entry', 0) != 0 else 0,
                    'nifty_above_50ma': trade.get('nifty_break', 0) > trade.get('nifty_50ma', 0),
                    'volume_spike': (trade.get('vol_at_break', 0) / trade.get('avg_vol', 1)) > 1.5,
                    'vol_vs_avg_pct_at_breakout': (trade.get('vol_at_break', 0) / trade.get('avg_vol', 1)) * 100,
                    'volume_multiplier': trade.get('vol_mult'),
                    'avg_daily_vol_20d': trade.get('avg_vol'),
                    'day_low_price': df_t['low'].min() if (df_t is not None and not df_t.empty) else 0,
                    'prev_day_high': prev_d['high'] if prev_d is not None else 0,
                    'prev_day_low': prev_d['low'] if prev_d is not None else 0,
                    'atr_14': trade.get('atr_val'),
                    'avg_intraday_range_pct_10d': trade.get('avg_range_10d'),
                    'is_bb_squeeze': trade.get('is_squeeze'),
                    'pivot': trade.get('pivots', {}).get('pivot'),
                    's1': trade.get('pivots', {}).get('s1'),
                    'r1': trade.get('pivots', {}).get('r1'),
                    'min_ma': trade.get('min_ma'), 
                    'max_ma': trade.get('max_ma'),
                    'daily_ma20': trade.get('daily_mas')[0], 'daily_ma50': trade.get('daily_mas')[1],
                    'daily_ma100': trade.get('daily_mas')[2], 'daily_ma200': trade.get('daily_mas')[3],
                    'hourly_ma20': trade.get('hourly_mas')[0], 'hourly_ma50': trade.get('hourly_mas')[1],
                    'hourly_ma100': trade.get('hourly_mas')[2], 'hourly_ma200': trade.get('hourly_mas')[3]
                })

    # Final Output with Safety Catch
    try:
        if final_results:
            output_df = pd.DataFrame(final_results)
            output_filename = "Paper_trading_week_16th_feb.xlsx"
            
            # Save the file
            output_df.to_excel(output_filename, index=False)
            
            logger.info("--------------------------------------------------")
            logger.info(f"SUCCESS: {len(final_results)} trades saved to {output_filename}")
            logger.info("--------------------------------------------------")
        else:
            logger.warning("SCAN COMPLETE: No breakout trades met the criteria. No file created.")
            
    except Exception as e:
        logger.error(f"CRITICAL ERROR saving Excel file: {e}")
        # Emergency backup save as CSV if Excel fails
        if final_results:
            pd.DataFrame(final_results).to_csv("EMERGENCY_BACKUP_RESULTS.csv", index=False)
            logger.info("Emergency backup saved as CSV.")

if __name__ == "__main__":
    run_scanner()
</file>

<file path="requirements.txt">
pandas
numpy
kiteconnect
supabase
python-dotenv
scikit-learn
joblib
google-generativeai
yfinance
requests
httpx
playwright
fastapi
pydantic
uvicorn
pyotp
gspread
oauth2client
selenium
webdriver-manager
</file>

<file path="utils/news_agent.py">
import yfinance as yf
import datetime
from utils.common import logger

def get_stock_news(symbol):
    """
    Fetches news for a given symbol from Yahoo Finance.
    Uses an aggressive search for titles and timestamps.
    """
    yf_symbol = f"{symbol}.NS"
    ticker = yf.Ticker(yf_symbol)
    
    try:
        news_list = ticker.news
        logger.info(f"[Yahoo News Debug] Raw news count for {yf_symbol}: {len(news_list) if news_list else 0}")
        if not news_list:
            return None

        recent_news = []
        now = datetime.datetime.now(datetime.timezone.utc)
        lookback_window = now - datetime.timedelta(days=1)

        for item in news_list:
            # --- Aggressive Title Search ---
            # Try top-level, then try inside the 'content' block
            title = item.get('title') or item.get('headline')
            if not title and 'content' in item:
                title = item.get('content', {}).get('title')
            
            # --- Aggressive Timestamp Search ---
            ts = item.get('providerPublishTime')
            if not ts and 'content' in item:
                ts = item.get('content', {}).get('pubDate')

            if title and ts:
                try:
                    # Handle int timestamps
                    if isinstance(ts, (int, float)):
                        publish_time = datetime.datetime.fromtimestamp(ts, datetime.timezone.utc)

                    # Handle ISO date strings (THIS WAS MISSING)
                    elif isinstance(ts, str):
                        publish_time = datetime.datetime.fromisoformat(ts.replace("Z", "+00:00"))

                    else:
                        continue

                    if publish_time > lookback_window:
                        recent_news.append(f"[{publish_time.strftime('%Y-%m-%d %H:%M')}] {title}")
                except Exception:
                    continue

        # Join results or return None if still empty
        return " | ".join(recent_news) if recent_news else None

    except Exception as e:
        logger.error(f"Error fetching news for {symbol}: {str(e)}")
        return None
</file>

<file path="utils/screenshot_scrapper.py">
import argparse
import asyncio
import os
from datetime import datetime
import shutil
import uuid
from pathlib import Path
from playwright.async_api import async_playwright
import time
import os
import json
from dotenv import load_dotenv
import base64

load_dotenv()

# --- RESTORED ORIGINAL CONSTANTS ---
LOGIN_CHECK_TIMEOUT = 120_000
NAV_TIMEOUT = 60_000
PAGE_LOAD_WAIT = 4_000       # Back to 4s (Reliable)
FULL_RENDER_WAIT = 2_000     # Back to 2s (Reliable)
VIEWPORT = {"width": 1600, "height": 900}
HEADLESS = True

TV_CHART_URL = "https://www.tradingview.com/chart/kJ0Io8nr/"

class Timer:
    def __init__(self, name):
        self.name = name
        self.start = None
    async def __aenter__(self):
        self.start = time.time()
        print(f"[timer] {self.name} started...")
        return self
    async def __aexit__(self, *args):
        print(f"[timer] {self.name} completed in {time.time() - self.start:.2f}s")


async def make_chart_screenshot(page, symbol, interval, target_dt, bars_needed, out_path, max_retries=2):
    """
    Captures a TradingView chart screenshot with retry logic.
    """
    for attempt in range(max_retries):
        try:
            step_start = time.time()
            
            # 1. Map interval to TradingView format
            tv_interval = interval
            if interval == "60":
                tv_interval = "60"  # 1h
            elif interval == "D":
                tv_interval = "1D"
            
            # 2. Construct URL using the WORKING approach from screenshot_scrapper_2
            safe_symbol = symbol.replace("&", "_")
            tv_symbol = f"NSE:{safe_symbol}" if ":" not in safe_symbol else safe_symbol

            chart_url = f"{TV_CHART_URL}?symbol={tv_symbol}&interval={tv_interval}"
            
            print(f"Loading {interval} chart for {symbol}... (Attempt {attempt + 1}/{max_retries})")

            # 3. Navigate with LONGER timeout (like screenshot_scrapper_2)
            await page.goto(chart_url, timeout=60000, wait_until="domcontentloaded")

            # Force clean layout
            await page.evaluate("""
                const selectors = [
                    'div[class*="header-toolbar"]',
                    'div[class*="side-toolbar"]',
                    '.logo',
                    '.watermark'
                ];

                const hideElements = document.querySelectorAll(selectors.join(', '));
                hideElements.forEach(el => {
                    el.style.display = 'none';
                });

                // Force dark
                document.documentElement.classList.add('theme-dark');
            """)

            await page.wait_for_timeout(2000)  # Let JS run

            print(f"[timer] [{interval}] Page goto: {time.time() - step_start:.2f}s")
            step_start = time.time()
            
            # 4. Wait for page to load completely
            await page.wait_for_timeout(4000)  # 4 seconds as in screenshot_scrapper_2
            print(f"[timer] [{interval}] Initial page load wait: {time.time() - step_start:.2f}s")
            step_start = time.time()

            # 5. Reset Chart View (Do this FIRST - from screenshot_scrapper_2)
            try:
                await page.keyboard.press("Alt+KeyR")
                await page.wait_for_timeout(1000)
            except:
                pass
            print(f"[timer] [{interval}] Alt+R jump: {time.time() - step_start:.2f}s")
            step_start = time.time()

            # 6. Final Render Wait
            await page.wait_for_timeout(3000)  # 1s + 2s as in screenshot_scrapper_2
            print(f"[timer] [{interval}] Final render wait: {time.time() - step_start:.2f}s")
            step_start = time.time()

            # 7. Capture Screenshot
            out_path.parent.mkdir(parents=True, exist_ok=True)

            try:
                await page.add_style_tag(content="""
                    div[class*="dialog-"], div[class*="modal-"], div[class*="popup-"], 
                    div[class*="overlay-"], div[data-role="dialog"], 
                    div[data-role="toast-container"], 
                    div[id^="overlap-manager-root"] div[class*="item-"] {
                        display: none !important;
                    }
                """)
                await page.keyboard.press("Escape")
                await page.wait_for_timeout(500)
            except: pass

            # Wait until indicators are actually mounted
            await page.wait_for_selector(
                'div[class*="legend"]',
                timeout=15000
            )
            
            await page.screenshot(
                path=str(out_path), 
                full_page=False,
                timeout=60000
            )
            
            print(f"[timer] [{interval}] Screenshot capture: {time.time() - step_start:.2f}s")
            print(f"[OK] Saved: {out_path.name}")
            return  # Success, exit function
            
        except Exception as e:
            if attempt < max_retries - 1:
                print(f"[RETRY] Screenshot failed (attempt {attempt + 1}/{max_retries}): {e}")
                await asyncio.sleep(3)  # Wait 3 seconds before retry
                continue
            else:
                print(f"[ERROR] Failed to capture screenshot for {symbol} ({interval}) after {max_retries} attempts: {e}")
                raise e

async def run(symbol: str, dt_str: str, out_dir: str):
    safe_symbol_for_file = symbol.replace(":", "_")
    target_dt = datetime.fromisoformat(dt_str)
    day_num = target_dt.day
    suffix = "th" if 11 <= day_num <= 13 else {1: "st", 2: "nd", 3: "rd"}.get(day_num % 10, "th")
    date_str = f"{day_num}{suffix}{target_dt.strftime('%b%Y')}"

    safe_symbol_for_file = symbol.replace(':', '_')
    # Treat out_dir as the final folder where screenshots are saved
    out_path = Path(out_dir)
    out_path.mkdir(parents=True, exist_ok=True)
    
    overall_start = time.time()
    print(f"[timer] ===== OVERALL START =====")

    try:
        async with async_playwright() as p:
            # Launch one context
            browser = await p.chromium.launch(
                headless=HEADLESS,
                channel="chrome",
                args=["--disable-blink-features=AutomationControlled"],
            )

            tv_state_b64 = os.getenv("TV_STATE_JSON_B64")

            if not tv_state_b64:
                raise Exception("TV_STATE_JSON_B64 not found in .env")

            decoded = base64.b64decode(tv_state_b64)
            tv_state_dict = json.loads(decoded)

            context = await browser.new_context(
                viewport=VIEWPORT,
                storage_state=tv_state_dict
            )

            page_15 = await context.new_page()
            page_60 = await context.new_page()
            page_D  = await context.new_page()
            
            async with Timer("Taking all 3 screenshots in parallel"):
                # Extract clean symbol name (remove exchange)
                symbol_short = symbol.split(":")[-1]

                await asyncio.gather(
                    make_chart_screenshot(
                        page_15, symbol, "15", target_dt, 3,
                        out_path / f"{symbol_short}_{date_str}_15m.png"
                    ),
                    make_chart_screenshot(
                        page_60, symbol, "60", target_dt, 12,
                        out_path / f"{symbol_short}_{date_str}_1h.png"
                    ),
                    make_chart_screenshot(
                        page_D, symbol, "D", target_dt, 90,
                        out_path / f"{symbol_short}_{date_str}_D.png"
                    ),
                )
            await context.close()
        
    except Exception as e:
        print(f"CRITICAL ERROR in screenshot run: {e}")
        # Re-raise to ensure the caller knows it failed
        raise e
    
    print(f"[timer] ===== OVERALL COMPLETED IN {time.time() - overall_start:.2f}s =====\n")

def parse_args():
    parser = argparse.ArgumentParser("TradingView Screenshot Agent")
    parser.add_argument("--symbol", required=True)
    parser.add_argument("--dt", required=True)
    parser.add_argument("--out", default="./out")
    parser.add_argument("--headless", action="store_true")
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
    HEADLESS = args.headless or HEADLESS
    asyncio.run(run(args.symbol, args.dt, args.out))
</file>

<file path="utils/slack_bot.py">
import os
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import httpx
from dotenv import load_dotenv

# ============================
# LOAD ENV
# ============================
load_dotenv()

SLACK_BOT_TOKEN = os.getenv("SLACK_BOT_TOKEN")
SLACK_CHANNEL_ID = os.getenv("SLACK_CHANNEL_ID")

if not SLACK_BOT_TOKEN:
    raise RuntimeError("Missing SLACK_BOT_TOKEN")

if not SLACK_CHANNEL_ID:
    raise RuntimeError("Missing SLACK_CHANNEL_ID")

# ============================
# APP
# ============================
app = FastAPI(title="Breakout Alert Bot")

# ============================
# SCHEMA
# ============================
class BreakoutAlert(BaseModel):
    symbol: str
    price: float
    timeframe: str
    confidence: float

# ============================
# ENDPOINT
# ============================
@app.post("/alert")
async def alert(alert: BreakoutAlert):
    message = (
        "üö® *BREAKOUT ALERT*\n\n"
        f"*Symbol:* {alert.symbol}\n"
        f"*Price:* {alert.price}\n"
        f"*Timeframe:* {alert.timeframe}\n"
        f"*Confidence:* {alert.confidence:.2f}\n\n"
        "üëÄ Check and keep your eye on this breakout."
    )

    headers = {
        "Authorization": f"Bearer {SLACK_BOT_TOKEN}",
        "Content-Type": "application/json",
    }

    payload = {
        "channel": SLACK_CHANNEL_ID,
        "text": message,
    }

    async with httpx.AsyncClient(timeout=10) as client:
        res = await client.post(
            "https://slack.com/api/chat.postMessage",
            headers=headers,
            json=payload,
        )

    data = res.json()

    if not data.get("ok"):
        raise HTTPException(
            status_code=500,
            detail=f"Slack API failed: {data}"
        )

    return {
        "status": "sent",
        "channel": data["channel"],
        "ts": data["ts"]
    }
</file>

<file path="DockerFile">
# Use an official Python runtime as a parent image
FROM python:3.10-slim

# Install Chrome and Chromedriver dependencies
RUN apt-get update && apt-get install -y \
    wget gnupg unzip \
    google-chrome-stable \
    && rm -rf /var/lib/apt/lists/*

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE 1
ENV PYTHONUNBUFFERED 1

# Set the working directory in the container
WORKDIR /app

# Install system dependencies
# These are required for httpx, pandas, and future selenium/playwright usage
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    curl \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/*

# Copy the requirements file into the container
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy the rest of the application code
COPY . .

# Expose the port the app runs on (Render uses $PORT environment variable)
EXPOSE 8000

# Start the application using uvicorn
# We use 0.0.0.0 to allow external connections from Render's proxy
CMD ["sh", "-c", "uvicorn main:app --host 0.0.0.0 --port ${PORT:-8000}"]
</file>

<file path="live_core_functions/monitor_breakouts.py">
import datetime
import pandas as pd
from utils.common import supabase, kite, logger, next_price_above


SCRIPT_START_TIME = datetime.datetime.now()

# -------------------------- Helper Functions -------------------------

def process_breakout(symbol, monitor_data, current_price, analysis_date, instrument_token):
    """Processes a potential breakout for a stock."""

    if not instrument_token:
        logger.warning(f"Skipping {symbol}: invalid or missing instrument_token")
        return "invalid_token", None
    
    max_ma = monitor_data.get("max_ma")
    if max_ma is None or pd.isna(max_ma):
        return "invalid_max_ma", None

    tick_size = monitor_data.get("tick_size") or 0.05
    breakout_price = next_price_above(max_ma, tick_size)

    if current_price < breakout_price:
        return "price_below_breakout", symbol

    existing = supabase.table("live_breakouts") \
        .select("symbol") \
        .eq("symbol", symbol) \
        .eq("breakout_date", analysis_date.strftime("%Y-%m-%d")) \
        .limit(1) \
        .execute()

    if existing.data:
        logger.debug(f"‚è≠Ô∏è Duplicate breakout ignored for {symbol}")
        return "duplicate", None
    
    # ============================================
    # BREAKOUT CHECK (8th MA = max_ma)
    # ============================================

    # Fetch minute data for the day
    try:
        minute_data = kite.historical_data(
            instrument_token=instrument_token,
            from_date=analysis_date,
            to_date=analysis_date,
            interval="minute"
        )
        if not minute_data:
            return "no_intraday_data", symbol
        df_intraday = pd.DataFrame(minute_data)
        df_intraday = df_intraday.rename(columns={"date": "date", "open": "open", "high": "high", "low": "low", "close": "close", "volume": "volume"})
        df_intraday["date"] = pd.to_datetime(df_intraday["date"]).dt.tz_localize(None)
    except Exception as e:
        logger.error(f"Failed to fetch minute data for {symbol}: {e}")
        return "no_intraday_data", symbol
    
    if df_intraday.empty:
        return "no_intraday_data", symbol
    
    df_intraday = df_intraday.set_index("date")

    # Check if price has crossed 8th MA (max_ma) = BREAKOUT!
    breakout_price = next_price_above(max_ma, tick_size)

    # Look back from the LAST CHECK TIME instead of current time
    entry_time_str = monitor_data.get("monitor_entry_time")
    if not entry_time_str:
        logger.error(f"{symbol}: monitor_entry_time missing ‚Äî cannot backfill breakout safely")
        return "invalid_monitor_state", None

    db_entry_time = pd.to_datetime(entry_time_str)
    # This ensures we only look for candles AFTER the script started
    last_check_time = max(db_entry_time, SCRIPT_START_TIME)
    
    breakout_rows = df_intraday[(df_intraday["high"] >= breakout_price) & (df_intraday.index >= last_check_time)]
    # --- CRITICAL FIX: LTP FALLBACK ---
    # If historical candles don't show breakout yet, but live LTP does, FORCE IT.
    if breakout_rows.empty and current_price >= breakout_price:
        logger.info(f"‚ö†Ô∏è {symbol} Candle lag detected! LTP ({current_price}) > MaxMA ({max_ma}). Forcing breakout.")
        
        # Create a synthetic row for "now" to allow processing to continue
        now_ts = datetime.datetime.now().replace(second=0, microsecond=0)
        
        # Only add if it doesn't exist
        if now_ts not in df_intraday.index:
            new_row = df_intraday.iloc[-1].copy() # Copy last known candle structure
            new_row["high"] = max(new_row["high"], current_price)
            new_row["close"] = current_price
            new_row["low"] = min(new_row["low"], current_price) # Adjust low if needed
            
            # Append to DataFrame
            df_intraday.loc[now_ts] = new_row

            breakout_rows = df_intraday[
            (df_intraday["high"] >= breakout_price) &
            (df_intraday.index >= last_check_time)
        ]

    if breakout_rows.empty:
        logger.debug(f"{symbol}: No breakout | Current: {current_price:.2f} | Breakout: {breakout_price:.2f} | Max High: {df_intraday['high'].max():.2f}")
        return "no_breakout", symbol
    
    breakout_time = breakout_rows.index[0]

    monitor_entry_time = pd.to_datetime(entry_time_str)

    logger.info(f"üöÄ BREAKOUT DETECTED: {symbol} crossed 8th MA ({max_ma:.2f}) at {breakout_time}")

    day_high = float(df_intraday["high"].max())
    early_breakout = {
        "symbol": symbol,
        "breakout_date": analysis_date.strftime("%Y-%m-%d"),
        "breakout_time": breakout_time.isoformat(),
        "breakout_price": float(breakout_price),
        "monitor_entry_time": monitor_entry_time.isoformat(),
        "high_price": day_high,
        "percent_move": round(((day_high - float(breakout_price)) / float(breakout_price)) * 100, 2)
    }

    supabase.table("live_breakouts").upsert(
        early_breakout,
        on_conflict="symbol,breakout_date"
    ).execute()

    return "success", early_breakout


def get_monitor_list(analysis_date, symbols=None):
    """Fetches today's monitor list from Supabase with pagination."""
    try:
        all_data = []
        page_size = 1000
        offset = 0
        
        while True:
            query = supabase.table("monitor_list").select(
                "symbol, min_ma, max_ma, open_price, tick_size, rsi_at_entry,monitor_entry_time, "
                "ma20_daily, ma50_daily, ma100_daily, ma200_daily, "
                "ma20_hourly, ma50_hourly, ma100_hourly, ma200_hourly, "
                "prev_day_high, prev_day_low, prev_day_close, "
                "atr_14, avg_daily_vol_20d, pivot_points, monitoring_tier, "
                "latest_news, is_bb_squeeze, avg_intraday_range_pct_10d"

            ).eq("date", analysis_date.strftime("%Y-%m-%d"))
            
            # Filter by symbols if provided
            if symbols:
                query = query.in_("symbol", symbols)
            
            # Add pagination
            query = query.range(offset, offset + page_size - 1)
            
            response = query.execute()
            
            if not response.data:
                break  # No more data
            
            all_data.extend(response.data)
            
            if len(response.data) < page_size:
                break  # Last page (fewer than 1000 rows returned)
            
            offset += page_size
        
        logger.info(f"Fetched {len(all_data)} stocks from monitor list for {analysis_date}")
        return all_data
        
    except Exception as e:
        logger.error(f"Failed to fetch monitor list: {e}")
        return []
</file>

<file path="utils/refresh_token.py">
import os
import time
import pyotp
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from kiteconnect import KiteConnect
from utils.common import supabase, KITE_API_KEY, KITE_API_SECRET

# Credentials from Render/Env
USER_ID = os.getenv("KITE_USER_ID")
PASSWORD = os.getenv("KITE_PASSWORD")
TOTP_SECRET = os.getenv("TOTP_SECRET")

def get_kite_access_token():
    kite = KiteConnect(api_key=KITE_API_KEY)
    login_url = kite.login_url()

    # 1. Setup Headless Chrome for Render environment
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    
    driver = webdriver.Chrome(options=chrome_options)

    try:
        driver.get(login_url)
        time.sleep(2)

        # 2. Login Flow
        driver.find_element(By.XPATH, '//input[@type="text"]').send_keys(USER_ID)
        driver.find_element(By.XPATH, '//input[@type="password"]').send_keys(PASSWORD)
        driver.find_element(By.XPATH, '//button[@type="submit"]').click()
        time.sleep(2)

        # 3. Use pyotp to bypass manual 2FA
        totp = pyotp.TOTP(TOTP_SECRET).now() # Generates the 6-digit code
        driver.find_element(By.XPATH, '//input[@type="number"]').send_keys(totp)
        driver.find_element(By.XPATH, '//button[@type="submit"]').click()
        time.sleep(5)

        # 4. Extract Request Token
        current_url = driver.current_url
        if "request_token=" not in current_url:
            print(f"‚ùå Failed to find token. Current URL: {current_url}")
            driver.save_screenshot("login_error.png") # Helpful for debugging on Render
            raise Exception("Token not found in URL")
        request_token = current_url.split("request_token=")[1].split("&")[0]
        
        # 5. Get the Session
        data = kite.generate_session(request_token, api_secret=KITE_API_SECRET)
        return data["access_token"]

    finally:
        driver.quit()

def update_supabase_token(token):
    if supabase:
        supabase.table("kite_config").upsert(
            {"key_name": "access_token", "value": token},
            on_conflict="key_name"
        ).execute()
        print(f"‚úÖ Access Token updated at {time.strftime('%H:%M:%S')}")

if __name__ == "__main__":
    try:
        new_token = get_kite_access_token()
        update_supabase_token(new_token)
    except Exception as e:
        print(f"‚ùå Automation failed: {e}")
</file>

<file path=".gitignore">
# Ignore environment files
.env

# Ignore all the log files
*.log

# Ignore the pycache
__pycache__/

# Ignore the Trading session folders
tradingview_session_15/
tradingview_session_60/
tradingview_session_D/
temp_sessions/
tradingview_warm_session/
tradingview_session_any/

# Ignore the screenshot folder
chart_screens/
temp/
screenshots/

# Ignore the venv folder
tvenv/

# Other files
access_token_fetch.py
tv_login_once.py
testing.py
testing_agent.py
tv_state.json
logistic_breakout_model_2.pkl
credentials.json
derivative_perfomance.py

# Ignore excel files
*.xlsx
*.pkl
</file>

<file path="live_core_functions/live_paper_trader.py">
import gspread
import pandas as pd
import datetime
import time
from utils.common import kite, token_map, load_token_map, logger, supabase, batch_upsert_supabase, next_price_above
import threading
from dotenv import load_dotenv
import json
import os
import base64
import pytz
from utils.common import get_ist_time

load_dotenv()
load_token_map()
# --- Google Sheets Setup ---
creds_b64 = os.getenv("GOOGLE_SHEET_CREDS_B64")

if not creds_b64:
    raise Exception("GOOGLE_SHEET_CREDS_B64 not set")

decoded = base64.b64decode(creds_b64)
info = json.loads(decoded)

gc = gspread.service_account_from_dict(info)
sh = gc.open("Live_Paper_Trading")
sheet = sh.sheet1

PAPER_TRADE_ONLY = True
SHEET_LOCK = threading.Lock()
ACTIVE_EXIT_MONITORS = set()
ARMED_TIMES = {}


def backfill_exit_for_open_trades():
    if sheet is None:
        return

    all_rows = sheet.get_all_values()
    today = datetime.date.today()

    for i, row in enumerate(all_rows[1:], start=2):

        if len(row) < 11:
            continue

        if row[10].strip() != "OPEN":
            continue

        symbol = row[1]
        buy_price = float(row[6])
        buy_time_str = row[0]   # Breakout Time column

        try:
            buy_dt = datetime.datetime.strptime(buy_time_str, "%Y-%m-%d %H:%M:%S")
        except:
            continue

        token = token_map.get(symbol)
        if not token:
            continue

        try:
            candles = kite.historical_data(
                token,
                from_date=buy_dt,
                to_date=datetime.date.today(),
                interval="minute"
            )
        except:
            continue

        if not candles:
            continue

        sl = buy_price
        target = buy_price * 1.03

        exit_price = None
        reason = None

        for c in candles:
            low = c["low"]
            high = c["high"]

            if high >= target:
                exit_price = target
                reason = "Target Hit 3%"
                break

            if low <= sl:
                exit_price = sl
                reason = "SL Hit Breakout Price"
                break


        if exit_price is None:
            exit_price = candles[-1]["close"]
            reason = "EOD Exit @15:15"

        pnl_pct = round(((exit_price - buy_price) / buy_price) * 100, 2)

        with SHEET_LOCK:
            sheet.update_cell(i, 11, "CLOSED")
            sheet.update_cell(i, 12, exit_price)
            sheet.update_cell(i, 13, reason)
            sheet.update_cell(i, 14, f"{pnl_pct}%")

        print(f"üìå Backfilled {symbol}: {reason}")

DAILY_BUDGET = 20000
daily_spent = 0

def place_sl_l_buy_order(symbol, trigger_price, limit_price, investment_per_trade=5000):
    """
    Places SL-L BUY order in Kite.
    Does NOT buy immediately.
    It will execute ONLY if price hits trigger_price.
    """

    tradingsymbol = symbol

    # Check LTP so Kite doesn't reject trigger order
    q = kite.quote([f"NSE:{tradingsymbol}"])
    ltp = q[f"NSE:{tradingsymbol}"]["last_price"]

    if ltp >= trigger_price:
        raise Exception(f"Skip arming: LTP {ltp} already >= trigger {trigger_price}")

    qty = int(investment_per_trade / limit_price)
    if qty <= 0:
        raise Exception("Quantity becomes 0 (investment too low for this stock price)")

    if PAPER_TRADE_ONLY:
        armed_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        ARMED_TIMES[symbol] = armed_time

        print(
            f"[PAPER MODE] ARMED SL-L BUY {symbol} | "
            f"trigger={trigger_price} limit={limit_price} investment={investment_per_trade}"
        )

        # OPTIONAL: log to Google Sheet so you can verify everything
        try:
            ok = update_armed_time(symbol, armed_time)
            if not ok:
                print(f"‚ÑπÔ∏è Trade row not created yet for {symbol}, armed time stored in memory.")

        except Exception as e:
            print(f"‚ö†Ô∏è Failed to log ARM to sheet: {e}")

        return "PAPER_ARMED"

    # order_id = kite.place_order(
    #     variety=kite.VARIETY_REGULAR,
    #     exchange=kite.EXCHANGE_NSE,
    #     tradingsymbol=tradingsymbol,
    #     transaction_type=kite.TRANSACTION_TYPE_BUY,
    #     quantity=qty,
    #     order_type=kite.ORDER_TYPE_SL,       # ‚úÖ SL-L order
    #     product=kite.PRODUCT_MIS,            # use MIS for intraday, CNC for delivery
    #     price=limit_price,                   # ‚úÖ limit price
    #     trigger_price=trigger_price,         # ‚úÖ trigger price
    #     validity=kite.VALIDITY_DAY
    # )

    # print(f"‚úÖ SL-L ARMED {symbol} qty={qty} trigger={trigger_price} limit={limit_price} order_id={order_id}")
    # return order_id

def update_armed_time(symbol, armed_time):
    if sheet is None:
        return False

    all_rows = sheet.get_all_values()
    for i, row in enumerate(all_rows[1:], start=2):  # start=2 because header row
        if len(row) >= 11 and row[1] == symbol and row[10] == "OPEN":
            with SHEET_LOCK:
                sheet.update_cell(i, 5, armed_time)  # Column E = 5
            return True
    return False

def reset_daily_budget():
    """Reset budget at market open"""
    global daily_spent
    daily_spent = 0

def update_sheet(data):
    if sheet is None:
        print("‚ùå Google Sheet not connected, cannot append.")
        return False

    for attempt in range(1, 6):
        try:
            with SHEET_LOCK:
                sheet.append_row(data)
            return True
        except Exception as e:
            print(f"‚ö†Ô∏è append_row failed attempt {attempt}/5: {e}")
            time.sleep(1.5 * attempt)

    return False

def update_trade_analysis(symbol, model_pred, ai_dec):
    """Finds the open trade in Google Sheets and fills in the AI/ML results."""
    if sheet is None: return
    try:
        all_rows = sheet.get_all_values()
        for i, row in enumerate(all_rows[1:], start=2):
            # Find the row for this symbol that is still 'OPEN'
            if len(row) >= 11 and row[1] == symbol and row[10].strip() == "OPEN":
                with SHEET_LOCK:
                    sheet.update_cell(i, 3, str(model_pred)) # Column C: ML Pred
                    sheet.update_cell(i, 4, str(ai_dec))    # Column D: AI Dec
                print(f"‚úÖ Analysis results filled in for {symbol}")
                break
    except Exception as e:
        print(f"Error updating analysis: {e}")

def start_paper_trade(symbol, breakout_price, breakout_time, model_pred, ai_dec):
    """
    Call this from monitor_breakouts.py when a cross is detected.
    """
    global daily_spent

    # Ensure breakout_time is used as the primary timestamp for the record
    if isinstance(breakout_time, datetime.datetime):
        breakout_dt = breakout_time
    else:
        breakout_dt = datetime.datetime.now() # Fallback
    
    buy_time = breakout_dt # Sync Buy Time to the Breakout Detection Time
    
    investment_per_trade = 5000
    quantity = int(investment_per_trade / breakout_price)
    money_traded = quantity * breakout_price

    if daily_spent + money_traded > DAILY_BUDGET:
        print(f"‚ùå Budget exhausted: ‚Çπ{daily_spent}/‚Çπ{DAILY_BUDGET}")
        return
    
    ai_dec = str(ai_dec)
    armed_time = ARMED_TIMES.get(symbol, "")

    row = [
        breakout_dt.strftime("%Y-%m-%d %H:%M:%S"),  # ‚úÖ Breakout Time (actual candle time)
        symbol,
        model_pred,                               # Model Prediction
        ai_dec,                                   # AI Decision
        armed_time,                               # E Armed Time
        buy_time.strftime("%H:%M:%S"),            # Buy Time
        breakout_price,                           # Buy Price
        breakout_price,                           # Actual Breakout Price
        quantity,                                 # ADD: Quantity
        money_traded,                             # ADD: Money Traded
        "OPEN"                                    # Status
    ]

    daily_spent += money_traded
    print(f"üí∞ Budget used: ‚Çπ{daily_spent}/‚Çπ{DAILY_BUDGET}")

    # First insert row
    ok = update_sheet(row)
    if not ok:
        print(f"‚ùå Sheet insert failed for {symbol}. Trade aborted.")
        return

    print(f"Successfully logged trade for {symbol} to Google Sheets.")

    # Then register monitor
    if symbol in ACTIVE_EXIT_MONITORS:
        return

    ACTIVE_EXIT_MONITORS.add(symbol)

    # Fetch token directly from Kite instruments
    instruments = kite.instruments("NSE")
    token = None

    for ins in instruments:
        if ins["tradingsymbol"] == symbol:
            token = ins["instrument_token"]
            break

    if not token:
        print(f"‚ùå Token not found for {symbol}. Cannot start exit monitor.")
        return

    exit_thread = threading.Thread(
        target=monitor_live_exit,
        args=(symbol, breakout_price, token),
        daemon=True
    )
    exit_thread.start()

def finalize_trade(symbol, exit_price, reason):
    """
    Finds the active trade in Google Sheets and updates it with exit data.
    """
    try:
        # 1. Fetch all rows to find the matching symbol with 'OPEN' status
        all_rows = sheet.get_all_values()
        row_num = -1
        
        for i, row in enumerate(all_rows):
            if len(row) >= 11 and row[1] == symbol and row[10].strip() == "OPEN":
                row_num = i + 1  # gspread uses 1-based indexing
                buy_price = float(row[6]) # Column index 5 is Buy Price
                break
        
        if row_num != -1:
            pnl_pct = round(((exit_price - buy_price) / buy_price) * 100, 2)

            # ‚úÖ Make updates thread-safe
            with SHEET_LOCK:
                sheet.update_cell(row_num, 11, "CLOSED")
                sheet.update_cell(row_num, 12, exit_price)
                sheet.update_cell(row_num, 13, reason)
                sheet.update_cell(row_num, 14, f"{pnl_pct}%")
            
            # Update Supabase live_breakouts with final EOD move and exit info
            try:
                supabase.table("live_breakouts").update({
                    "percent_move": pnl_pct,
                    "high_price": float(exit_price),
                    "exit_reason": reason
                }).eq("symbol", symbol).eq("breakout_date", datetime.date.today().strftime("%Y-%m-%d")).execute()
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to update Supabase with final move for {symbol}: {e}")

            print(f"Successfully finalized {symbol}: {reason} at {exit_price} ({pnl_pct}%)")

            # ‚úÖ COMPLETE cleanup for re-entry
            ACTIVE_EXIT_MONITORS.discard(symbol)
            from live_core_functions.minute_monitor_stocks import PAPER_TRADES_TODAY, ARMED_SYMBOLS
            PAPER_TRADES_TODAY.discard(symbol)
            ARMED_SYMBOLS.discard(symbol)  # ‚úÖ ADD THIS LINE

        else:
            print(f"Error: Could not find an active 'OPEN' trade for {symbol}.")
            ACTIVE_EXIT_MONITORS.discard(symbol)
            from live_core_functions.minute_monitor_stocks import PAPER_TRADES_TODAY, ARMED_SYMBOLS
            PAPER_TRADES_TODAY.discard(symbol)
            ARMED_SYMBOLS.discard(symbol)

    except Exception as e:
        print(f"CRITICAL ERROR in finalize_trade: {e}")

def monitor_live_exit(symbol, buy_price, token):
    sl = buy_price
    target = round(buy_price * 1.03, 2)
    
    while symbol in ACTIVE_EXIT_MONITORS:
        try:
            # üÜï CHANGE 1: Use live quote instead of historical candles
            quote = kite.quote([f"NSE:{symbol}"])
            curr_price = quote[f"NSE:{symbol}"]["last_price"]
        
            now = get_ist_time()
            
            # Exit 1: SL Hit
            if curr_price <= sl:
                finalize_trade(symbol, sl, "SL Hit Breakout Price")
                break
            
            # Exit 2: Target Hit
            elif curr_price >= target:
                finalize_trade(symbol, target, "Target Hit 3%")
                break
            
            # Exit 3: EOD Exit (Redundant safety)
            elif now.time() >= datetime.time(15, 15):
                finalize_trade(symbol, curr_price, "EOD Exit @15:15")
                break
                
            time.sleep(2) # Faster polling for live data
        except Exception as e:
            logger.error(f"Error monitoring {symbol}: {e}")
            time.sleep(5)
</file>

<file path="utils/common.py">
import os
import logging
import sys
from decimal import Decimal, ROUND_CEILING
from dotenv import load_dotenv
from supabase import create_client, Client
from kiteconnect import KiteConnect
import math
import json
import time
try:
    import numpy as np
except Exception:
    np = None
import pytz
from datetime import datetime

# -------------------------- Configuration & Setup --------------------------

# Load .env file
load_dotenv()

# Setup Logger
# This configures the root logger. Other files can just get their own logger.
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s',
    handlers=[
        logging.FileHandler('app_shared.log', encoding='utf-8'), # A shared log file
        logging.StreamHandler()
    ]
)
# Get a logger for this common module
logger = logging.getLogger(__name__)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)

# Load Environment Variables
SUPABASE_URL = os.getenv("K_SUPABASE_URL")
SUPABASE_KEY = os.getenv("K_SUPABASE_KEY")
KITE_API_KEY = os.getenv("KITE_API_KEY")
KITE_API_SECRET = os.getenv("KITE_API_SECRET")
KITE_ACCESS_TOKEN = os.getenv("KITE_ACCESS_TOKEN")

# Validate Environment Variables
if not all([SUPABASE_URL, SUPABASE_KEY]):
    logger.critical("Required environment variables are not set. Exiting.")
    # Use sys.exit() in a shared module to stop everything if config is missing
    sys.exit("Critical Error: Missing environment variables.")

def get_ist_time():
    """Returns the current time in Indian Standard Time (IST)."""
    ist = pytz.timezone('Asia/Kolkata')
    return datetime.now(ist).replace(tzinfo=None)

def get_active_token():
    """Fetches the latest token from Supabase or falls back to env."""
    try:
        # Explicitly check for the supabase object
        if supabase is not None:
            res = supabase.table("kite_config") \
                .select("value") \
                .eq("key_name", "access_token") \
                .execute()
            
            if res.data and len(res.data) > 0:
                return res.data[0]["value"]
    except Exception as e:
        logger.warning(f"Could not fetch token from Supabase: {e}")
    
    # Fallback to .env if DB query fails or returns nothing
    return os.getenv("KITE_ACCESS_TOKEN")

# -------------------------- Client Initialization --------------------------

try:
    import httpx
    from supabase import create_client, Client

    logger.info("Testing direct Supabase HTTP connectivity...")

    try:
        response = httpx.get(
            f"{SUPABASE_URL}/rest/v1/monitor_list",
            headers={
                "apikey": SUPABASE_KEY,
                "Authorization": f"Bearer {SUPABASE_KEY}"
            },
            params={"select": "symbol", "limit": 1},
            verify=False,
            timeout=5
        )

        logger.info(f"Supabase HTTP status: {response.status_code}")

    except Exception as e:
        logger.warning(f"Direct Supabase HTTP check failed: {e}")

    # -------------------------------
    # 2Ô∏è‚É£ Initialize Supabase Client
    # -------------------------------
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

    # Disable SSL verification (Windows workaround)
    supabase.postgrest.session = httpx.Client(verify=False)

    # üîç Warmup query (REAL health check)
    try:
        supabase.table("monitor_list").select("symbol").limit(1).execute()
        logger.info("Supabase connectivity OK")
    except Exception as e:
        logger.error(
            f"Supabase unreachable at startup (will use Excel fallback): {e}"
        )
        logger.warning("Will retry Supabase on actual operations...")
        supabase = None

    # 3Ô∏è‚É£ Initialize Kite Client
    kite = KiteConnect(api_key=KITE_API_KEY)
    active_token = get_active_token()
    kite.set_access_token(active_token)

    try:
        kite.profile()
        logger.info("Successfully initialized Kite client.")
    except Exception as e:
        logger.error(f"Kite session expired or invalid: {e}. Automation script should refresh it.")

except Exception as e:
    logger.critical(f"Failed to initialize critical clients: {e}")
    sys.exit("Critical Error: Client initialization failed.")

# -------------------------- Helper Functions --------------------------

def next_price_above(value, tick):
    """Calculates the next valid price tick above a given value."""
    dv = Decimal(str(value))
    dt = Decimal(str(tick))
    n = (dv / dt).to_integral_value(rounding=ROUND_CEILING)
    candidate = n * dt
    if candidate <= dv:
        candidate = (n + 1) * dt
    return float(candidate)

def to_json_safe(value):
    """
    Convert values to JSON-safe:
    - NaN/Inf -> None
    - numpy scalars -> python scalars
    - recursively handle dict/list
    """
    if value is None:
        return None

    # numpy scalar -> python scalar
    if np is not None:
        try:
            if isinstance(value, (np.floating, np.integer, np.bool_)):
                value = value.item()
        except Exception:
            pass

    # float NaN/Inf -> None
    if isinstance(value, float):
        if math.isnan(value) or math.isinf(value):
            return None

    if isinstance(value, dict):
        return {k: to_json_safe(v) for k, v in value.items()}

    if isinstance(value, list):
        return [to_json_safe(v) for v in value]

    return value

def batch_upsert_supabase(table_name, data, batch_size=100):
    """Upserts data into Supabase in batches with retry & sanitization."""
    if not data:
        logger.info("No data to upsert.")
        return

    MAX_RETRIES = 3

    for i in range(0, len(data), batch_size):
        batch = data[i:i + batch_size]

        # ‚úÖ sanitize NaN / Inf / numpy
        batch = [{k: to_json_safe(v) for k, v in row.items()} for row in batch]

        # strict JSON validation
        json.dumps(batch, allow_nan=False)

        for attempt in range(1, MAX_RETRIES + 1):
            try:
                if table_name == "live_breakouts":
                    conflict_key = "symbol,breakout_date"
                elif table_name == "live_ml_features":
                    conflict_key = "symbol,date"
                else:
                    conflict_key = "symbol,date"

                if supabase is None:
                    raise Exception("Supabase client not available")

                supabase.table(table_name).upsert(
                    batch, on_conflict=conflict_key
                ).execute()

                logger.info(
                    f"Upserted batch {i//batch_size + 1} to {table_name}."
                )
                break  # ‚úÖ success ‚Üí exit retry loop

            except Exception as e:
                logger.error(
                    f"[batch_upsert_supabase] Attempt {attempt}/{MAX_RETRIES} "
                    f"failed for {table_name}: {e}"
                )

                if attempt == MAX_RETRIES:
                    logger.error(
                        f"[batch_upsert_supabase] Giving up batch "
                        f"{i//batch_size + 1} ({len(batch)} rows)"
                    )
                    raise Exception("Supabase batch upsert failed")
                else:
                    time.sleep(2* attempt)

# -------------------------- Token Map --------------------------

token_map = {}

def load_token_map():
    global token_map
    try:
        instruments = kite.instruments("NSE")
        token_map = {ins["tradingsymbol"]: ins["instrument_token"] for ins in instruments}
        logger.info(f"Loaded {len(token_map)} NSE instrument tokens.")
        return token_map
    except Exception as e:
        logger.error(f"Failed to load token map: {e}")
        return {}
</file>

<file path="live_core_functions/build_monitor_list.py">
import datetime
import pandas as pd
import numpy as np
import time
import gc
from utils.common import supabase, kite, logger, batch_upsert_supabase, next_price_above, get_active_token
import io
import requests
from utils.news_agent import get_stock_news
from concurrent.futures import ThreadPoolExecutor

# -------------------------- Helper Functions -------------------------
def get_kite_token_map():
    """Builds symbol -> instrument_token map from Kite"""
    return {
        ins["tradingsymbol"]: ins["instrument_token"]
        for ins in kite.instruments("NSE")
        if ins.get("instrument_token")
    }

def get_derivative_symbols():
    """Fetches all symbols currently available in the F&O segment."""
    try:
        # Fetching NFO instruments to see which symbols have derivatives
        nfo_ins = kite.instruments("NFO")
        # Use a set for O(1) lookup speed
        return {ins["tradingsymbol"] for ins in nfo_ins}
    except Exception as e:
        logger.error(f"Failed to fetch NFO instruments: {e}")
        return set()
    
def get_all_nse_symbols():
    """Fetch all active NSE mainboard EQ symbols directly from official NSE CSV."""
    logger.info("Fetching NSE symbol list...")
    url = "https://archives.nseindia.com/content/equities/EQUITY_L.csv"
    
    # Browser-like headers to avoid blocking
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'
    }
    
    max_retries = 3
    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            csv_data = io.StringIO(response.text)
            df = pd.read_csv(csv_data)
            break
        except Exception as e:
            if attempt < max_retries - 1:
                logger.warning(f"Retry {attempt+1}/{max_retries} failed to fetch symbol list: {e}")
                time.sleep(5)
            else:
                logger.error(f"All retries failed to fetch symbol list: {e}")
                return []  # Return empty on final failure
            
    # Get the list of F&O symbols        
    fno_symbols = get_derivative_symbols()
    # Clean and filter
    df.columns = df.columns.str.strip()
    df = df[df['SERIES'] == 'EQ']
    
    # Return in format expected by your code: list of dicts with 'symbol'
    symbols = [{"symbol": row['SYMBOL'], "tick_size": 0.05} for _, row in df.iterrows()]


    kite_token_map = get_kite_token_map()

    enriched_symbols = []
    missing_tokens = 0

    for s in symbols:
        token = kite_token_map.get(s["symbol"])
        if not token:
            missing_tokens += 1
            continue  # skip symbols not tradable on Kite

        enriched_symbols.append({
            "symbol": s["symbol"],
            "instrument_token": token,
            "tick_size": s.get("tick_size", 0.05),
            "has_derivative": s["symbol"] in fno_symbols
        })

    logger.info(f"Symbols dropped due to missing Kite token: {missing_tokens}")
    logger.info(f"Final tradable NSE symbols: {len(enriched_symbols)}")

    return enriched_symbols
    
def calculate_rsi(series, period=14):
    """Calculate RSI manually using pandas."""
    delta = series.diff()
    gain = delta.where(delta > 0, 0.0)
    loss = -delta.where(delta < 0, 0.0)
    avg_gain = gain.rolling(window=period, min_periods=period).mean()
    avg_loss = loss.rolling(window=period, min_periods=period).mean()
    rs = avg_gain / avg_loss
    rsi = 100 - (100 / (1 + rs))
    return rsi

def calculate_atr(df, period=14):
    """Calculate Average True Range (ATR)."""
    if df.empty or not all(c in df.columns for c in ['high', 'low', 'close']):
        return np.nan
    
    # Ensure we are working with a copy to avoid SettingWithCopyWarning
    df_copy = df.copy()
    
    df_copy['high_low'] = df_copy['high'] - df_copy['low']
    df_copy['high_prev_close'] = np.abs(df_copy['high'] - df_copy['close'].shift(1))
    df_copy['low_prev_close'] = np.abs(df_copy['low'] - df_copy['close'].shift(1))
    
    tr = df_copy[['high_low', 'high_prev_close', 'low_prev_close']].max(axis=1)
    atr = tr.rolling(window=period, min_periods=period).mean()
    
    # Return the last value
    return atr.iloc[-1] if not atr.empty else np.nan

def calculate_pivots(prev_high, prev_low, prev_close):
    """Calculate standard pivot points."""
    if pd.isna(prev_high) or pd.isna(prev_low) or pd.isna(prev_close):
        return {}
    
    pivot = (prev_high + prev_low + prev_close) / 3
    s1 = (pivot * 2) - prev_high
    r1 = (pivot * 2) - prev_low
    s2 = pivot - (prev_high - prev_low)
    r2 = pivot + (prev_high - prev_low)
    
    return {
        'pivot': round(pivot, 2),
        's1': round(s1, 2),
        'r1': round(r1, 2),
        's2': round(s2, 2),
        'r2': round(r2, 2)
    }

def calculate_bb_squeeze(df, period=20, std_dev=2, kc_period=20, kc_atr_mult=1.5):
    """Detect Bollinger Band squeeze using BB vs Keltner Channel."""
    if df.empty or len(df) < period:
        return False

    df = df.copy()
    sma = df["close"].rolling(period).mean()
    std = df["close"].rolling(period).std()
    bb_upper = sma + std_dev * std
    bb_lower = sma - std_dev * std
    bb_width = bb_upper - bb_lower

    atr = calculate_atr(df, kc_period)
    kc_middle = df["close"].rolling(kc_period).mean()
    kc_upper = kc_middle.iloc[-1] + kc_atr_mult * atr if pd.notna(atr) else np.nan
    kc_lower = kc_middle.iloc[-1] - kc_atr_mult * atr if pd.notna(atr) else np.nan
    kc_width = kc_upper - kc_lower

    if pd.notna(bb_width.iloc[-1]) and pd.notna(kc_width):
        return bb_width.iloc[-1] < kc_width
    return False

def preload_daily_data(stocks, from_date, to_date):
    """Fetches ONLY daily data."""
    data_dict = {}
    for stock in stocks:
        symbol = stock["symbol"]
        token = stock["instrument_token"]
        
        retries = 3
        for attempt in range(retries):
            try:
                data = kite.historical_data(instrument_token=token, from_date=from_date, to_date=to_date, interval="day")
                if data:
                    df = pd.DataFrame(data)
                    df = df.rename(columns={"date": "date", "open": "open", "high": "high", "low": "low", "close": "close", "volume": "volume"})
                    df["date"] = pd.to_datetime(df["date"]).dt.tz_localize(None)
                    data_dict[symbol] = df[["date", "open", "high", "low", "close", "volume"]]
                break
            except Exception as e:
                wait_time = 1 if "429" not in str(e) else 2
                if attempt < retries - 1:
                    time.sleep(wait_time) 
                else:
                    logger.error(f"Failed to fetch daily for {symbol}: {e}")
        time.sleep(0.05) # Respect Kite rate limit
    return data_dict

def preload_hourly_data(stocks, from_date, to_date):
    """Fetches ONLY hourly data for stocks that survived the daily filter."""
    data_dict = {}
    hourly_start = max(from_date, to_date - datetime.timedelta(days=100))
    for stock in stocks:
        symbol = stock["symbol"]
        token = stock["instrument_token"]
        
        retries = 3
        for attempt in range(retries):
            try:
                data = kite.historical_data(instrument_token=token, from_date=hourly_start, to_date=to_date, interval="60minute")
                if data:
                    df = pd.DataFrame(data)
                    df = df.rename(columns={"date": "date", "open": "open", "high": "high", "low": "low", "close": "close", "volume": "volume"})
                    df["date"] = pd.to_datetime(df["date"]).dt.tz_localize(None)
                    data_dict[symbol] = df.sort_values(by="date").drop_duplicates(subset="date").reset_index(drop=True)[["date", "open", "high", "low", "close", "volume"]]
                break
            except Exception as e:
                wait_time = 1 if "429" not in str(e) else 2
                if attempt < retries - 1:
                    time.sleep(wait_time) 
                else:
                    logger.error(f"Failed to fetch hourly for {symbol}: {e}")
        time.sleep(0.05)
    return data_dict

def precompute_mas(data_dict, stocks, today_open_prices):
    """Precomputes 8 MAs for each stock using history + today's open."""
    ma_dict = {}
    for stock in stocks:
        symbol = stock["symbol"]
        if symbol not in data_dict:
            continue
        ma_dict[symbol] = {}
        for interval in ["daily", "hourly"]:
            source_df = data_dict[symbol].get(interval)
            if source_df is None or source_df.empty or "close" not in source_df.columns:
                logger.warning(f"Skipping MA for {symbol} ({interval}): Invalid data.")
                continue
            dates = source_df["date"].tolist()
            prices = source_df["close"].tolist()
            if symbol in today_open_prices and today_open_prices[symbol] > 0:
                dates.append(pd.to_datetime(datetime.date.today()))
                prices.append(today_open_prices[symbol])
            ma_df = pd.DataFrame({"date": dates})
            price_series = pd.Series(prices)

            for period in [20, 50, 100, 200]:
                if len(price_series) >= period:
                    ma_df[f"ma{period}"] = price_series.rolling(window=period).mean()
                else:
                    ma_df[f"ma{period}"] = pd.NA

            ma_dict[symbol][interval] = ma_df.copy()
    return ma_dict

def process_batch(batch_info):
    batch_stocks, batch_idx, total_batches, from_date, to_date, auth_token = batch_info
    yesterday = to_date - datetime.timedelta(days=1)

    # 1. AUTHENTICATE KITE
    if hasattr(kite, "set_access_token"):
        kite.set_access_token(auth_token)
    elif hasattr(kite, "enctoken"):
        kite.enctoken = auth_token
        if hasattr(kite, "headers"):
            kite.headers["Authorization"] = f"enctoken {auth_token}"

    logger.info(f"Processing batch {batch_idx}/{total_batches}...")
    monitor_list = []
    
    # --- OPTIMIZATION 1: QUOTES FIRST ---
    # We only care about stocks that have valid live prices.
    symbols = [s["symbol"] for s in batch_stocks]
    try:
        quote_keys = [f"NSE:{s}" for s in symbols]
        raw_quotes = kite.quote(quote_keys)
        
        quotes = {}
        valid_stocks = []
        for stock in batch_stocks:
            sym = stock["symbol"]
            q_data = raw_quotes.get(f"NSE:{sym}", {})
            last_price = q_data.get("last_price") or q_data.get("ltp")
            open_price = q_data.get("open") or q_data.get("ohlc", {}).get("open")
            
            if last_price and open_price and open_price > 0:
                quotes[sym] = {"last_price": last_price, "open": open_price}
                valid_stocks.append(stock)
                
        if not valid_stocks:
            return monitor_list
    except Exception as e:
        logger.error(f"Batch {batch_idx}: Failed to fetch quotes: {e}")
        return monitor_list

    # --- OPTIMIZATION 2: DAILY FILTER ---
    # Fetch Daily data ONLY to check the Slingshot condition.
    daily_data_dict = preload_daily_data(valid_stocks, from_date, yesterday)
    
    surviving_stocks = []
    daily_mas_cache = {}
    
    for stock in valid_stocks:
        symbol = stock["symbol"]
        if symbol not in daily_data_dict:
            continue
            
        df_daily = daily_data_dict[symbol]
        current_open = quotes[symbol]["open"]
        
        # Calculate Daily MAs including today's open
        prices = df_daily["close"].tolist()
        prices.append(current_open)
        price_series = pd.Series(prices)
        
        daily_mas = []
        is_valid = True
        for p in [20, 50, 100, 200]:
            if len(price_series) >= p:
                ma_val = price_series.rolling(window=p).mean().iloc[-1]
                if pd.isna(ma_val) or ma_val <= 0:
                    is_valid = False
                    break
                daily_mas.append(float(ma_val))
            else:
                is_valid = False
                break
                
        if not is_valid:
            continue
            
        # MATH FILTER: If open >= min(daily MAs), it can't be a Slingshot.
        if current_open >= min(daily_mas):
            continue 
            
        surviving_stocks.append(stock)
        daily_mas_cache[symbol] = daily_mas

    surviving_set = {s["symbol"] for s in surviving_stocks}
    for sym in list(daily_data_dict.keys()):
        if sym not in surviving_set:
            del daily_data_dict[sym]
    gc.collect()

    # --- OPTIMIZATION 3: HOURLY FETCH ONLY FOR SURVIVORS ---
    # This is where the real time-saving happens.
    hourly_data_dict = preload_hourly_data(surviving_stocks, from_date, yesterday)

    surviving_symbols = [s["symbol"] for s in surviving_stocks]
    with ThreadPoolExecutor(max_workers=5) as executor:
        news_map = dict(zip(surviving_symbols, executor.map(get_stock_news, surviving_symbols)))

    for stock in surviving_stocks:
        symbol = stock["symbol"]
        if symbol not in hourly_data_dict:
            continue
            
        df_hourly = hourly_data_dict[symbol]
        current_open = quotes[symbol]["open"]
        current_ltp = quotes[symbol]["last_price"]
        
        # Calculate Hourly MAs including today's open
        h_prices = df_hourly["close"].tolist()
        h_prices.append(current_open)
        h_price_series = pd.Series(h_prices)
        
        hourly_mas = []
        is_valid = True
        for p in [20, 50, 100, 200]:
            if len(h_price_series) >= p:
                ma_val = h_price_series.rolling(window=p).mean().iloc[-1]
                if pd.isna(ma_val) or ma_val <= 0:
                    is_valid = False
                    break
                hourly_mas.append(float(ma_val))
            else:
                is_valid = False
                break
        
        if not is_valid:
            continue

        # Final MA Check
        all_mas = daily_mas_cache[symbol] + hourly_mas
        min_ma = min(all_mas)
        
        if current_open >= min_ma:
            continue

        # Stock is valid! Build final entry
        df_daily = daily_data_dict[symbol]
        prev_day = df_daily.iloc[-1]
        pivots = calculate_pivots(prev_day["high"], prev_day["low"], prev_day["close"])
        recent_daily = df_daily.tail(20)
        atr_14 = calculate_atr(recent_daily)
        
        is_bb_squeeze = calculate_bb_squeeze(df_daily.tail(30)) if len(df_daily) >= 30 else False
        df_hist_10 = df_daily.tail(10).copy()
        df_hist_10["range_pct"] = ((df_hist_10["high"] - df_hist_10["low"]) / df_hist_10["low"]) * 100
        avg_range = float(df_hist_10["range_pct"].mean())

        rsi_at_entry = calculate_rsi(df_daily["close"]).iloc[-1] if len(df_daily) >= 15 else None
        stock_news = news_map.get(symbol)

        monitor_entry = {
            "symbol": symbol,
            "date": datetime.date.today().strftime("%Y-%m-%d"),
            "latest_news": stock_news,
            "open_price": float(current_open),
            "current_price": float(current_ltp),
            "min_ma": float(min_ma),
            "max_ma": float(max(all_mas)),
            "monitoring_tier": "slow",
            "tick_size": float(stock["tick_size"]),
            "ma20_daily": daily_mas_cache[symbol][0],
            "ma50_daily": daily_mas_cache[symbol][1],
            "ma100_daily": daily_mas_cache[symbol][2],
            "ma200_daily": daily_mas_cache[symbol][3],
            "ma20_hourly": hourly_mas[0],
            "ma50_hourly": hourly_mas[1],
            "ma100_hourly": hourly_mas[2],
            "ma200_hourly": hourly_mas[3],
            "prev_day_high": float(prev_day["high"]),
            "prev_day_low": float(prev_day["low"]),
            "prev_day_close": float(prev_day["close"]),
            "pivot_points": pivots,
            "atr_14": float(atr_14) if pd.notna(atr_14) else None,
            "avg_daily_vol_20d": float(recent_daily["volume"].mean()),
            "rsi_at_entry": float(rsi_at_entry) if pd.notna(rsi_at_entry) else None,
            "is_bb_squeeze": bool(is_bb_squeeze),
            "avg_intraday_range_pct_10d": avg_range,
            "monitor_entry_time": datetime.datetime.combine(to_date, datetime.time(9, 15)).isoformat(),
            "has_derivative": bool(stock.get("has_derivative", False)),
        }
        print(f"‚úÖ FOUND: {symbol} | Open: {current_open} | MinMA: {min_ma:.2f}")
        monitor_list.append(monitor_entry)

    return monitor_list

def create_monitor_list():
    """Builds monitor list sequentially to save memory on Render."""
    logger.info("Starting Memory-Optimized Monitor List Builder...")
    
    logger.info("Starting Memory-Optimized Monitor List Builder...")
    
    # üîÑ FORCE SYNC TOKEN FROM DB BEFORE STARTING
    auth_token = get_active_token()
    
    if not auth_token:
        logger.error("‚ùå No token available in DB. Monitor list build aborted.")
        return

    # Update the global kite client instance
    kite.set_access_token(auth_token)

    # 2. Set Dates (Target = Today)
    analysis_date = datetime.date.today()
    from_date = analysis_date - datetime.timedelta(days=365)
    
    # 3. Fetch Stocks (ALL Stocks)
    stocks = get_all_nse_symbols()
    if not stocks:
        logger.error("No stocks fetched. Exiting.")
        return
    

    # 4. Process Sequentially in Small Batches (Reduced to 50 for RAM safety)
    batch_size = 100
    total_stocks = len(stocks)
    total_batches = (total_stocks + batch_size - 1) // batch_size
    
    logger.info(f"Processing {total_stocks} stocks in {total_batches} sequential batches...")

    for i in range(0, total_stocks, batch_size):
        batch = stocks[i:i + batch_size]
        batch_idx = (i // batch_size) + 1
        
        # Prepare inputs for the existing process_batch function
        batch_input = (batch, batch_idx, total_batches, from_date, analysis_date, auth_token)
        
        try:
            # üÜï CRITICAL: Run sequentially, NOT in a Pool
            batch_monitor_list = process_batch(batch_input)
            
            # Upsert this batch immediately to Supabase
            if batch_monitor_list:
                batch_upsert_supabase("monitor_list", batch_monitor_list)
                logger.info(f"‚úÖ Batch {batch_idx}/{total_batches} saved and cleared from RAM.")
            
            # üÜï CRITICAL: Clear batch data from memory immediately
            del batch_monitor_list
            gc.collect() 
            
        except Exception as e:
            logger.error(f"‚ùå Batch {batch_idx} failed: {e}")

    logger.info("üèÅ Monitor List Builder completed.")
if __name__ == "__main__":
    create_monitor_list()
</file>

<file path="live_core_functions/minute_monitor_stocks.py">
import time as time_module
from datetime import datetime, date, time
import pandas as pd
import numpy as np
from utils.common import kite, logger, supabase, batch_upsert_supabase, next_price_above
import threading
from live_core_functions.monitor_breakouts import get_monitor_list, process_breakout, SCRIPT_START_TIME
from live_core_functions.live_paper_trader import start_paper_trade
import pytz
from utils.common import get_ist_time

ARMED_SYMBOLS = set()
PAPER_TRADES_TODAY = set()
TRADE_LOCK = threading.Lock()

def load_monitor_list_from_excel():
    try:
        df = pd.read_excel("monitor_list_backup.xlsx")
        return df.to_dict("records")
    except Exception as e:
        logger.error(f"Failed to load monitor list from Excel: {e}")
        return []

def get_stocks_by_tier(tier, date):
    """Fetch ALL stocks by monitoring tier using pagination with tier verification."""

    if supabase is None:
        logger.warning("Supabase down ‚Üí using Excel monitor list")
        stocks = load_monitor_list_from_excel()
        return [s["symbol"] for s in stocks if s.get("monitoring_tier") == tier]

    try:
        all_stocks = []
        page_size = 1000
        offset = 0
        
        while True:
            response = supabase.table("monitor_list")\
                .select("symbol, monitoring_tier")\
                .eq("date", date.strftime("%Y-%m-%d"))\
                .eq("monitoring_tier", tier)\
                .range(offset, offset + page_size - 1)\
                .execute()
            
            if not response.data:
                break
            
            # Double-check tier matches (handles race conditions from tier upgrades)
            for row in response.data:
                if row.get("monitoring_tier") == tier:
                    all_stocks.append(row["symbol"])
            
            if len(response.data) < page_size:
                break  # Last page
            
            offset += page_size
        
        logger.info(f"Fetched {len(all_stocks)} stocks for {tier} tier (verified)")
        return all_stocks
        
    except Exception as e:
        logger.error(f"Failed to fetch {tier} tier stocks: {e}")
        return []

def run_breakout_check(symbols, tier):
    """Trigger breakout monitoring for given symbols."""
    if not symbols:
        return
    
    logger.info(f"Checking {len(symbols)} stocks ({tier} tier) for breakouts...")
    
    today = date.today()

    # Use the global token_map from common instead of re-fetching every minute
    from utils.common import token_map as global_map
    if not global_map:
        from utils.common import load_token_map
        load_token_map()
    token_map_local = global_map
    
    # 1. Get LTP for ALL symbols in batches of 50 (Kite limit)
    all_quotes = {}
    for i in range(0, len(symbols), 50):
        batch = [f"NSE:{s}" for s in symbols[i:i+50]]
        try:
            all_quotes.update(kite.quote(batch))
        except Exception as e:
            logger.error(f"Quote batch failed: {e}")

    # Get Nifty data
    nifty_token = token_map_local.get("NIFTY 50")

    if nifty_token:
        try:
            nifty_data_raw = kite.historical_data(
                instrument_token=nifty_token,
                from_date=today,
                to_date=today,
                interval="minute"
            )
            nifty_df = pd.DataFrame(nifty_data_raw)
            if not nifty_df.empty:
                nifty_df["date"] = pd.to_datetime(nifty_df["date"]).dt.tz_localize(None)
                nifty_data = nifty_df.set_index("date")[["open", "close"]]
            else:
                nifty_data = pd.DataFrame()
        except:
            nifty_data = pd.DataFrame()
    else:
        nifty_data = pd.DataFrame()
    
    # 3. Fetch monitor data for these symbols
    try:
        fresh_monitor_list = get_monitor_list(today, symbols=symbols)
    except Exception as e:
        logger.error(f"Supabase monitor_list fetch failed: {e}")
        fresh_monitor_list = []

    # üîÅ Excel fallback
    if not fresh_monitor_list:
        logger.warning("Using Excel fallback for monitor list")
        fresh_monitor_list = load_monitor_list_from_excel()


    # Create a mapping of current tiers
    monitor_by_symbol = {stock["symbol"]: stock for stock in fresh_monitor_list}
    
    for symbol in symbols:
        # Get latest monitor data
        stock_data = monitor_by_symbol.get(symbol)
        if not stock_data:
            continue
        
        quote = all_quotes.get(f"NSE:{symbol}")
        if not quote: 
            continue
        
        current_price = quote["last_price"]
        
        # Use FRESH tier from database, not the stale one from function parameter
        current_tier = stock_data.get("monitoring_tier", tier)
        
        # --- TIER MANAGEMENT (UPGRADE & DOWNGRADE) ---
        mas = sorted([
            val
            for p in [20, 50, 100, 200]
            for t in ['daily', 'hourly']
            for val in [stock_data.get(f"ma{p}_{t}")]
            if val is not None and not pd.isna(val)
        ])

        if not mas: continue
        
        max_ma = mas[-1]                     # 8th MA (highest)
        seventh_ma = mas[-2] if len(mas) >= 2 else mas[-1]  # 7th MA (2nd highest)
        sixth_ma = mas[-3] if len(mas) >= 3 else seventh_ma # 6th MA (3rd highest)
        fifth_ma = mas[-4] if len(mas) >= 4 else mas[0]     # 5th MA

        # Upgrade Slow -> Fast
        if current_price >= sixth_ma and current_tier == "slow":
            try:
                supabase.table("monitor_list").update({"monitoring_tier": "fast"}).eq("symbol", symbol).eq("date", today.strftime("%Y-%m-%d")).execute()
                logger.info(f"‚ö° {symbol} upgraded to FAST (LTP {current_price} >= 6th MA {sixth_ma})")
            except: pass

            instrument_token = token_map_local.get(symbol)
            if not instrument_token:
                continue
            # ============================================
            # üöÄ PRECOMPUTE ML FEATURES AT FAST TIER
            # ============================================

            # analysis_date = datetime.date.today()

            # existing = supabase.table("live_ml_features") \
            #     .select("symbol") \
            #     .eq("symbol", symbol) \
            #     .eq("date", analysis_date.strftime("%Y-%m-%d")) \
            #     .execute()

            # if not existing.data:
            #     logger.info(f"‚ö° Precomputing ML features for FAST tier: {symbol}")

            #     stock_df = fetch_30d_daily_ohlcv(symbol, instrument_token, analysis_date)

            #     if stock_df is not None and not stock_df.empty:
            #         nifty_map = fetch_nifty_daily_map(analysis_date)

            #         rows = []
            #         for _, r in stock_df.iterrows():
            #             d = r["date"]
            #             n = nifty_map.get(d)
            #             rows.append({
            #                 "open": r["open"],
            #                 "high": r["high"],
            #                 "low": r["low"],
            #                 "close": r["close"],
            #                 "volume": r["volume"],
            #                 "nifty_open": n["open"] if n is not None else 0.0,
            #                 "nifty_close": n["close"] if n is not None else 0.0
            #             })

            #         ml_df = pd.DataFrame(rows)
            #         nifty_ml_df = ml_df[["nifty_open", "nifty_close"]]

            #         ml_features = get_ml_model_features(ml_df, nifty_ml_df)

            #     else:
            #         ml_features = {feat: 0.0 for feat in ML_FEATURES}

            #     supabase.table("live_ml_features").upsert(
            #         {
            #             "symbol": symbol,
            #             "date": analysis_date.strftime("%Y-%m-%d"),
            #             "features": ml_features
            #         },
            #         on_conflict="symbol,date"
            #     ).execute()

            #     logger.info(f"‚úÖ ML features cached early for {symbol}")

        # Downgrade Fast -> Slow
        if current_price <= fifth_ma and current_tier == "fast":
            try:
                supabase.table("monitor_list").update({"monitoring_tier": "slow"}).eq("symbol", symbol).eq("date", today.strftime("%Y-%m-%d")).execute()
                logger.info(f"üê¢ {symbol} downgraded to SLOW (LTP {current_price} <= 5th MA {fifth_ma})")
            except: pass    


        # ============================================================
        # ‚ö° ARM SL-L ORDER when LTP reaches midpoint of 7th & 8th MA
        # Only for FAST tier stocks
        # ============================================================
        if current_tier == "fast" and symbol not in ARMED_SYMBOLS:
            midpoint = (seventh_ma + max_ma) / 2
            tick_size = stock_data.get("tick_size") or 0.05
            arm_threshold = midpoint - tick_size

            if current_price >= arm_threshold:
                # 1. Define the prices BEFORE the try block
                breakout_price = next_price_above(max_ma, tick_size)
                trigger_price = breakout_price
                limit_price = round(breakout_price + tick_size, 2)

                try:
                    from live_core_functions.live_paper_trader import place_sl_l_buy_order
                    place_sl_l_buy_order(symbol, trigger_price, limit_price, investment_per_trade=5000)
                    ARMED_SYMBOLS.add(symbol)
                    logger.info(f"üü¶ ARMED {symbol} | TRIGGER={trigger_price} LIMIT={limit_price}")
                except Exception as e:
                    if "already >= trigger" in str(e):
                        ARMED_SYMBOLS.add(symbol)
                        logger.debug(f"Symbol {symbol} already past trigger, skipping further arming.")
                    else:
                        logger.error(f"‚ùå Failed to ARM order for {symbol}: {e}")

        # ============================================================
        # üöÄ IMMEDIATE LTP BREAKOUT TRIGGER
        # ============================================================
        tick_size = stock_data.get("tick_size") or 0.05
        breakout_price = next_price_above(max_ma, tick_size)

        # Trigger trade IMMEDIATELY when price crosses level
        if current_price >= breakout_price:
            # üÜï TIME GATE: Prevent new entries at or after 3:15 PM IST
            # Using get_ist_time() to ensure timezone consistency
            now_ist = get_ist_time()
            if now_ist.time() >= time(15, 15):
                logger.info(f"‚è≠Ô∏è Skipping late breakout for {symbol} at {current_price} (Time: {now_ist.strftime('%H:%M')})")
                continue

            # ‚úÖ Thread-safe check and add
            with TRADE_LOCK:
                if symbol in PAPER_TRADES_TODAY:
                    continue
                PAPER_TRADES_TODAY.add(symbol)
            
            # Check database to prevent duplicate trades for the same day
            existing = supabase.table("live_breakouts") \
                .select("symbol") \
                .eq("symbol", symbol) \
                .eq("breakout_date", today.strftime("%Y-%m-%d")) \
                .limit(1).execute()

            if not existing.data:
                detection_time = datetime.now()

                if detection_time <= SCRIPT_START_TIME:
                    logger.info(f"‚è≠Ô∏è Skipping old breakout for {symbol} (before script start)")
                    with TRADE_LOCK:
                        PAPER_TRADES_TODAY.discard(symbol)  # Allow re-check next cycle
                    continue

                logger.info(f"üéØ BREAKOUT DETECTED: {symbol} at {current_price}")
                start_paper_trade(
                    symbol=symbol,
                    breakout_price=breakout_price,
                    breakout_time=detection_time, # Pass the exact detection time
                    model_pred="Processing...",
                    ai_dec="Processing..."
                )

                # 2. RUN ANALYSIS IN BACKGROUND THREAD
                # This keeps the main loop fast so it can check other stocks
                analysis_thread = threading.Thread(target=process_breakout, kwargs={
                    "symbol": symbol,
                    "monitor_data": stock_data,
                    "current_price": current_price,
                    "analysis_date": today,
                    "instrument_token": token_map_local.get(symbol)
                })
                analysis_thread.daemon = True
                analysis_thread.start()


def check_stagnant_exits(now_str):
    """Exits trades where price has hit a circuit or stopped moving."""
    try:
        # 1. Fetch all active trades for today - REMOVED 'id' from select
        active_trades = supabase.table("live_breakouts") \
            .select("symbol, last_price_checked, stagnant_count, breakout_price") \
            .eq("breakout_date", now_str) \
            .is_("exit_reason", None) \
            .execute()

        if not active_trades.data:
            return

        # 2. Get live quotes for all active symbols
        symbols = [f"NSE:{t['symbol']}" for t in active_trades.data]
        quotes = kite.quote(symbols)

        for trade in active_trades.data:
            symbol = trade['symbol']
            q = quotes.get(f"NSE:{symbol}")
            if not q: continue

            current_ltp = q['last_price']
            last_ltp = trade['last_price_checked']
            count = trade['stagnant_count'] or 0

            # 3. Check if price is stagnant
            if last_ltp is not None and current_ltp == last_ltp:
                count += 1
                logger.info(f"‚ö†Ô∏è {symbol} is stagnant. Count: {count}/3 (LTP: {current_ltp})")
            else:
                count = 0 # Reset if price moves

            if count >= 3:
                logger.info(f"üõë EXIT: {symbol} hit stagnant/circuit limit at {current_ltp}")
                from live_core_functions.live_paper_trader import finalize_trade
                finalize_trade(symbol, current_ltp, "Stagnant/Circuit Exit")
                continue

            # 4. Update tracking columns using symbol and date as the unique identifier
            # We use breakout_date because your unique constraint is (symbol, breakout_date)
            supabase.table("live_breakouts").update({
                "last_price_checked": current_ltp,
                "stagnant_count": count
            }).eq("symbol", symbol).eq("breakout_date", now_str).execute()

    except Exception as e:
        logger.error(f"Error in stagnant check: {e}")

def start_finding_breakouts():
    """Main monitoring loop with non-blocking Slow Tier."""
    logger.info("Starting tiered monitoring system...")
    
    today = date.today()
    market_open = time(9, 15)
    market_close = time(15, 30)
    
    last_slow_check = None
    last_fast_check = None
    
    # Flag to prevent overlapping slow checks
    slow_check_running = False

    def run_slow_tier_background():
        nonlocal slow_check_running, last_slow_check
        try:
            slow_check_running = True
            slow_stocks = get_stocks_by_tier("slow", today)
            logger.info(f"üê¢ SLOW tier fetched {len(slow_stocks)} stocks")

            if slow_stocks:
                run_breakout_check(slow_stocks, "slow")
            last_slow_check = get_ist_time()
        except Exception as e:
            logger.error(f"Error in Slow Tier thread: {e}")
        finally:
            slow_check_running = False

    while True:
        now = get_ist_time()
        current_time = now.time()
        
        # Only run during market hours
        if current_time < market_open or current_time > market_close:
            logger.info("Market closed. Sleeping...")
            time_module.sleep(600)
            continue

        # 0. STAGNANT CHECK: Run every minute
        if last_fast_check is None or (now - last_fast_check).seconds >= 60:
            check_stagnant_exits(today.strftime("%Y-%m-%d"))
            
        # 1. FAST TIER: Run immediately in main thread (High Priority)
        if last_fast_check is None or (now - last_fast_check).seconds >= 60:
            fast_stocks = get_stocks_by_tier("fast", today)
            fast_stocks = [
                s for s in fast_stocks
                if s not in PAPER_TRADES_TODAY
            ]
            if fast_stocks:
                logger.info(f"‚ö° FAST tier stocks: ({len(fast_stocks)}):{fast_stocks} ")
                run_breakout_check(fast_stocks, "fast")
            last_fast_check = now
        
        # 2. SLOW TIER: Run every 15 minutes (900 seconds)
        if (last_slow_check is None or (now - last_slow_check).total_seconds() >= 900) and not slow_check_running:
            # We update the timestamp HERE before starting to prevent race conditions
            last_slow_check = now 
            t = threading.Thread(target=run_slow_tier_background)
            t.daemon = True
            t.start()
        
        time_module.sleep(5)

        if current_time >= time(15, 15) and current_time < time(15, 30):
            try:
                # Fetch all OPEN trades for today
                open_trades = supabase.table("live_breakouts") \
                    .select("symbol, breakout_price") \
                    .eq("breakout_date", now.strftime("%Y-%m-%d")) \
                    .is_("exit_reason", None) \
                    .execute()

                if open_trades.data:
                    logger.info(f"‚è∞ EOD Cleanup: Closing {len(open_trades.data)} remaining trades.")
                    for b in open_trades.data:
                        symbol = b["symbol"]
                        # Get final LTP
                        try:
                            q = kite.quote(f"NSE:{symbol}")
                            ltp = q[f"NSE:{symbol}"]["last_price"]
                        except:
                            ltp = b["breakout_price"] # Fallback

                        # This calls the finalize_trade helper to update Sheet and Supabase
                        from live_core_functions.live_paper_trader import finalize_trade
                        finalize_trade(symbol, ltp, "EOD Exit @15:15")
            except Exception as e:
                logger.error(f"Global EOD Force Exit Error: {e}")

        # EOD SNAPSHOT at 15:30 ‚Äî Save exact UI state for historical view
        if current_time >= time(15, 30):
            snapshot_taken = supabase.table("dashboard_snapshots") \
                .select("date") \
                .eq("date", today.strftime("%Y-%m-%d")) \
                .execute()

            if not snapshot_taken.data:   # Only take once per day
                try:
                    from main import build_dashboard_data   # Import the helper we created
                    snapshot_data = build_dashboard_data(today.strftime("%Y-%m-%d"))

                    supabase.table("dashboard_snapshots").upsert({
                        "date": today.strftime("%Y-%m-%d"),
                        "data": snapshot_data
                    }).execute()

                    logger.info(f"üì∏ Dashboard snapshot saved for {today} at 15:30")
                except Exception as e:
                    logger.error(f"Snapshot failed: {e}")

if __name__ == "__main__":
    start_finding_breakouts()
</file>

<file path="main.py">
import requests
import uvicorn
import os
import time
from fastapi import FastAPI, BackgroundTasks, HTTPException
import httpx
from pydantic import BaseModel
from live_core_functions.build_monitor_list import create_monitor_list
from live_core_functions.minute_monitor_stocks import start_finding_breakouts
from fastapi.responses import PlainTextResponse
from fastapi.middleware.cors import CORSMiddleware
import gspread
import base64
import json
from utils.common import supabase, logger
from datetime import date, datetime
from utils.refresh_token import get_kite_access_token, update_supabase_token
from utils.common import kite

creds_b64 = os.getenv("GOOGLE_SHEET_CREDS_B64")

if not creds_b64:
    raise Exception("GOOGLE_SHEET_CREDS_B64 not set")

decoded = base64.b64decode(creds_b64)
info = json.loads(decoded)

gc = gspread.service_account_from_dict(info)
sh = gc.open("Live_Paper_Trading")
sheet = sh.sheet1

app = FastAPI(title="Trademate")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ‚ö†Ô∏è Use only for development!
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class PayloadRequest(BaseModel):
    token: str

@app.post("/refresh-kite-session")
async def trigger_token_refresh(background_tasks: BackgroundTasks):
    """
    Endpoint for cron-job.org to refresh the Kite access token.
    Runs in background to avoid cron-job.org timeout.
    """
    def task_with_retries():
        max_retries = 5
        for attempt in range(1, max_retries + 1):
            try:
                print("üîÑ Automation: Starting headless login...")
                new_token = get_kite_access_token()
                update_supabase_token(new_token)
                kite.set_access_token(new_token)
                print("‚úÖ Automation: Token refresh successful.")
            except Exception as e:
                print(f"‚ùå Automation: Refresh failed: {e}")
                if attempt < max_retries:
                    wait_time = attempt * 30 # Wait 30s, 60s, 90s...
                    print(f"üïí Waiting {wait_time} seconds before next retry...")
                    time.sleep(wait_time)
                else:
                    print("‚ùå Automation: All 5 refresh attempts failed.")

    background_tasks.add_task(task_with_retries)
    return {"status": "request_received", "message": "Token refresh started in background."}

@app.post("/start-finding-breakouts")
async def handle_find_breakouts(background_tasks: BackgroundTasks):
    """
    Triggers the build and monitor flow in the background to prevent 
    HTTP timeouts and handle memory spikes safely.
    """
    def full_algo_flow():
        try:
            today_str = date.today().strftime("%Y-%m-%d")

            # üîÑ RE-FETCH AND FORCE SET TOKEN
            token_response = supabase.table("kite_config") \
                .select("value") \
                .eq("key_name", "access_token") \
                .execute()

            if token_response.data and len(token_response.data) > 0:
                new_token = token_response.data[0]["value"]
                kite.set_access_token(new_token)
                logger.info(f"‚úÖ Token synced from DB (starts with {new_token[:5]}...)")
            else:
                logger.error("‚ùå CRITICAL: No access_token found in kite_config table.")
                return # Stop the flow if no token exists

            # 1. Check/Build monitor list inside the background task
            existing = supabase.table("monitor_list") \
                .select("symbol", count="exact") \
                .eq("date", today_str) \
                .limit(1) \
                .execute()

            if not existing.count:
                print(f"üîÑ {today_str}: No monitor list found. Building now...")
                create_monitor_list()
            else:
                print(f"‚úÖ {today_str}: Monitor list already exists. Skipping build.")

            # 2. Start monitoring immediately after build completes
            start_finding_breakouts()
            
        except Exception as e:
            print(f"‚ùå Background Algo Flow Error: {e}")

    # Add the combined task to background
    background_tasks.add_task(full_algo_flow)

    return {
        "status": "success", 
        "message": "Monitor list build and breakout monitoring started in background."
    }

@app.get("/health", tags=["Health"])
async def health_check():
    return PlainTextResponse("OK", status_code=200)

@app.get("/api/dashboard")
def get_dashboard(date: str):
    requested_date = date  # e.g. "2026-02-23"

    # === 1. If today ‚Üí always return LIVE data ===
    today_str = datetime.today().strftime("%Y-%m-%d")
    if requested_date == today_str:
        return build_dashboard_data(requested_date)   # ‚Üê we'll create this helper below

    # === 2. For past dates ‚Üí return frozen snapshot ===
    try:
        snapshot = supabase.table("dashboard_snapshots") \
            .select("data") \
            .eq("date", requested_date) \
            .limit(1) \
            .execute()

        if snapshot.data:
            return snapshot.data[0]["data"]   # Return exactly what was saved at 3:30 PM
        else:
            # No snapshot yet (very old date) ‚Üí return empty or live as fallback
            return build_dashboard_data(requested_date)
    except:
        return build_dashboard_data(requested_date)
    
    
def build_dashboard_data(date_str: str):
    """All the existing dashboard logic moved here so we can reuse it."""
    result = {}
    
    # 1. Monitor list count
    try:
        r = supabase.table("monitor_list").select("symbol", count="exact").eq("date", date_str).execute()
        result["monitor_list_count"] = r.count
    except:
        result["monitor_list_count"] = 0

    # 2. Slow tier count
    try:
        r = supabase.table("monitor_list").select("symbol", count="exact").eq("date", date_str).eq("monitoring_tier", "slow").execute()
        result["slow_tier_count"] = r.count
    except:
        result["slow_tier_count"] = 0
        
    # 3. Fast tier count + symbols
    try:
        r = supabase.table("monitor_list") \
            .select("symbol,has_derivative, max_ma, tick_size", count="exact") \
            .eq("date", date_str) \
            .eq("monitoring_tier", "fast") \
            .execute()

        result["fast_tier_count"] = r.count

        fast_symbols_enriched = []
        for row in (r.data or []):
            max_ma = row.get("max_ma")
            tick_size = row.get("tick_size") or 0.05
            breakout_price = None
            if max_ma:
                from decimal import Decimal, ROUND_CEILING
                dv = Decimal(str(max_ma))
                dt = Decimal(str(tick_size))
                n = (dv / dt).to_integral_value(rounding=ROUND_CEILING)
                candidate = n * dt
                if candidate <= dv:
                    candidate = (n + 1) * dt
                breakout_price = float(candidate)
            
            fast_symbols_enriched.append({
                "symbol": row["symbol"],
                "breakout_price": round(breakout_price, 2) if breakout_price else None,
                "current_price": None,
                "has_derivative": bool(row.get("has_derivative", False))
            })

        result["fast_tier_symbols"] = fast_symbols_enriched

        if fast_symbols_enriched:
            try:
                symbols_list = [s["symbol"] for s in fast_symbols_enriched]
                from utils.common import kite
                all_quotes = {}
                for i in range(0, len(symbols_list), 50):
                    batch = [f"NSE:{s}" for s in symbols_list[i:i+50]]
                    quotes = kite.quote(batch)
                    all_quotes.update(quotes)
                
                for item in result["fast_tier_symbols"]:
                    sym = item["symbol"]
                    q = all_quotes.get(f"NSE:{sym}")
                    if q:
                        item["current_price"] = q.get("last_price")
            except Exception as e:
                pass
    except:
        result["fast_tier_count"] = 0
        result["fast_tier_symbols"] = []

    # 4. Breakout details with Live Price fetching
    try:
        r = supabase.table("live_breakouts") \
            .select("symbol, breakout_price, breakout_time, percent_move, high_price, exit_reason") \
            .eq("breakout_date", date_str) \
            .execute()

        breakouts_data = r.data or []
        
        # --- NEW: Fetch live prices for these breakouts ---
        if breakouts_data:
            breakout_symbols = [f"NSE:{b['symbol']}" for b in breakouts_data]
            try:
                # Fetch quotes from Kite for all breakout stocks
                live_quotes = kite.quote(breakout_symbols)
                
                for b in breakouts_data:
                    sym_key = f"NSE:{b['symbol']}"
                    if sym_key in live_quotes:
                        # Add current_price to the object sent to frontend
                        b["current_price"] = live_quotes[sym_key].get("last_price")
                    else:
                        b["current_price"] = b["breakout_price"] # Fallback
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to fetch live prices for breakouts: {e}")

        result["breakouts"] = breakouts_data
        for b in result["breakouts"]:
            exit_reason = b.get("exit_reason") or ""
            if "Target" in exit_reason:
                b["status"] = "Target Hit"
            elif "SL" in exit_reason:
                b["status"] = "SL Hit"
            elif "Stagnant" in exit_reason:
                b["status"] = "Circuit/Stagnant"
            elif "EOD" in exit_reason:
                b["status"] = "EOD Exit"
            else:
                b["status"] = "In Play"
        result["breakout_count"] = len(result["breakouts"])

    except Exception as e:
        print(f"‚ùå Error building breakout dashboard: {e}")
        result["breakouts"] = []
        result["breakout_count"] = 0

    # 5. Exit conditions + PnL from Google Sheet
    sl_hit = 0
    target_hit = 0
    eod_exit = 0
    pnl_list = []

    if sheet:
        try:
            all_rows = sheet.get_all_values()
            for row in all_rows[1:]:
                if len(row) < 14:
                    continue
                row_date = row[0][:10] if row[0] else ""
                if row_date != date_str:
                    continue
                exit_reason = row[12].strip() if len(row) > 12 else ""
                if "SL" in exit_reason:
                    sl_hit += 1
                elif "Target" in exit_reason:
                    target_hit += 1
                elif "EOD" in exit_reason:
                    eod_exit += 1
                pnl_str = row[13].replace("%", "").strip() if len(row) > 13 else "0"
                try:
                    pnl_list.append({
                        "symbol": row[1], 
                        "pnl": float(pnl_str),
                        "percent_move": float(pnl_str)
                    })
                except:
                    pass
        except:
            pass

    result["exit_conditions"] = {
        "sl_hit": sl_hit,
        "target_hit": target_hit,
        "eod_exit": eod_exit
    }

    overall_pnl = round(sum(p["pnl"] for p in pnl_list) / len(pnl_list), 2) if pnl_list else 0
    result["pnl"] = {
        "overall": overall_pnl,
        "stocks": pnl_list
    }

    return result
</file>

</files>
